{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pybullet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB6SkSb78G6_",
        "outputId": "3e9cb632-100c-42ad-9125-e06cef812c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (90.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 90.8 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "QBGrjvQs5ZoU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU on:', True if torch.cuda.is_available() else False, '| Device:',DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRTSDnr3WCm9",
        "outputId": "c7f52b4e-8525-4f8f-9a7d-ecd706e9e219"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU on: True | Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr+1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition) \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        sample_data = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "\n",
        "        states_ = []\n",
        "        next_states_ = [] \n",
        "        actions_ = []\n",
        "        rewards_ = []\n",
        "        dones_= []\n",
        "        for i in sample_data:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            states_.append(state)\n",
        "            next_states_.append(next_state)\n",
        "            actions_.append(action)\n",
        "            rewards_.append(reward)\n",
        "            dones_.append(done)\n",
        "        \n",
        "        return np.array(states_), np.array(next_states_), np.array(actions_), np.array(rewards_).reshape(-1,1), np.array(dones_).reshape(-1,1)"
      ],
      "metadata": {
        "id": "MAzkCTVL8Cz6"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input, action, cut):#action is the number outputs | output is the number of actions\n",
        "        super(Actor,self).__init__()\n",
        "        self.fully01 = nn.Linear(input, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.last = nn.Linear(300, action)\n",
        "        self.cut = cut\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fully01(x))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        return self.cut * torch.tanh(self.last(x))#the cut to adjust to the output levels. higher or lower that -1,1                             "
      ],
      "metadata": {
        "id": "zaWQLv_E-19F"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since we need two pair of critics, im making both on the same class.\n",
        "#the name of the class should be DoubleCritic,PairCritic..etc\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input, action):\n",
        "        super(Critic, self).__init__()\n",
        "        #first\n",
        "        self.fully01 = nn.Linear(input+action, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.fully03 = nn.Linear(300, 1)\n",
        "\n",
        "        #second\n",
        "        self.fully11 = nn.Linear(input+action, 400)\n",
        "        self.fully22 = nn.Linear(400,300)\n",
        "        self.fully33 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        #first\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        #Second\n",
        "        y = F.relu(self.fully11(xu))\n",
        "        y = F.relu(self.fully22(y))\n",
        "        y = self.fully33(y)\n",
        "        return x, y\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NgrMw3mE-24P"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "class TD3(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        # actors\n",
        "        self.Actor = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target.load_state_dict(self.Actor.state_dict())\n",
        "\n",
        "        # actor optimizer\n",
        "        self.Actor_optimizer = torch.optim.Adam(self.Actor.parameters())\n",
        "\n",
        "        ## Critic \n",
        "        self.Critic = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target.load_state_dict(self.Critic.state_dict())\n",
        "\n",
        "        ## Critic optimizer\n",
        "        self.Critic_optimizer = torch.optim.Adam(self.Critic.parameters())\n",
        "\n",
        "        ### Max_Action is the cut/clip\n",
        "        self.max_action =  max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1,-1)).to(DEVICE)\n",
        "        return self.Actor(state).cpu().data.numpy().flatten()\n",
        "        #return self.Actor(state).data.numpy().flatten()\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.Actor.state_dict(),'%s/%s_Actor.pth' % (directory,filename))\n",
        "        torch.save(self.Critic.state_dict(),'%s/%s_Critic.pth' % (directory,filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.Actor.load_state_dict(torch.load('%s/%s_Actor.pth' % (directory,filename)))\n",
        "        self.Critic.load_state_dict(torch.load('%s/%s_Critic.pth' % (directory,filename)))\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for i in range(iterations):\n",
        "            states_, next_states_, actions_, rewards_, dones_ = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.Tensor(states_).to(DEVICE)\n",
        "            next_state = torch.Tensor(next_states_).to(DEVICE)\n",
        "            action = torch.Tensor(actions_).to(DEVICE)\n",
        "            reward = torch.Tensor(rewards_).to(DEVICE)\n",
        "            done = torch.Tensor(dones_).to(DEVICE)\n",
        "\n",
        "            next_action = self.Actor_Target(next_state)\n",
        "\n",
        "            noise = torch.Tensor(actions_).data.normal_(0, policy_noise).to(DEVICE)\n",
        "            noise = noise.clamp(-noise_clip,noise_clip)\n",
        "            next_action = (next_action+noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            Target_Q1, Target_Q2 = self.Critic_Target(next_state, next_action)\n",
        "            \n",
        "            #when episode is over 1, not over 0. \n",
        "            # we detached because adding the reward which is the output \n",
        "            #of nn to the computaional graph would not be what we want.\n",
        "            Target_Q = torch.min(Target_Q1, Target_Q2)\n",
        "            Target_Q = reward + (discount * Target_Q * (1 - done)).detach()\n",
        "          \n",
        "            Current_Q1, Current_Q2 = self.Critic(state, action)\n",
        "            critic_loss = F.mse_loss(Current_Q1,Target_Q) + F.mse_loss(Current_Q2, Target_Q)\n",
        "\n",
        "            self.Critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.Critic_optimizer.step()\n",
        "\n",
        "            if not i % policy_freq:\n",
        "                actor_loss = -self.Critic.Q1(state, self.Actor(state)).mean()\n",
        "                self.Actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.Actor_optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.Actor.parameters(), self.Actor_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "                \n",
        "                for param, target_param in zip(self.Critic.parameters(), self.Critic_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)"
      ],
      "metadata": {
        "id": "YvN3Pdw2VKXr"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy, episodes=10):\n",
        "    avg_awards = 0\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_awards += reward\n",
        "    avg_awards /= episodes\n",
        "    print(f\"Average award over {episodes} is:\",avg_awards)\n",
        "    return avg_awards"
      ],
      "metadata": {
        "id": "uRc2bSmh91_o"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "env_name = 'HalfCheetahBulletEnv-v0'\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps= 6e5\n",
        "\n",
        "save_model = True\n",
        "expi_noise = 0.1\n",
        "batch_size = 100\n",
        "discount = 0.99\n",
        "tau = 0.005\n",
        "\n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5\n",
        "policy_freq = 2"
      ],
      "metadata": {
        "id": "syR2bCx7VKce"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "U6NG65vZVKej"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = f\"TD3--{env_name}--seed({seed})\"\n",
        "print(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iPqcoSTvxYR",
        "outputId": "8688082e-27ce-4ec5-8626-f7462ea98434"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD3--HalfCheetahBulletEnv-v0--seed(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./results'):\n",
        "    os.makedirs('./results')\n",
        "if save_model and not os.path.exists('./pytorch_models'):\n",
        "    os.makedirs('./pytorch_models')"
      ],
      "metadata": {
        "id": "lULFXN0t0bci"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting env\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "metadata": {
        "id": "9u7zaZkd1Oqf"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "metadata": {
        "id": "hafzat8h1Osy"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = ReplayBuffer() "
      ],
      "metadata": {
        "id": "KQv8Y-Q_1Ou4"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = [evaluate_policy(policy)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug1NGAnn1OxH",
        "outputId": "a7213447-9af3-41d8-e5c9-5c8fcf7d9dc9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average award over 10 is: -1429.4266421472653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "    \n",
        "work_dir = mkdir('exp','brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_step = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "G1HcVeyp1OzO"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing the variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "VCpTqgBc1O1v"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while total_timesteps < max_timesteps:\n",
        "    if done:\n",
        "        if total_timesteps != 0:\n",
        "            print(f'Total timesteps:{total_timesteps} - Episode num:{episode_num} - Reward:{episode_reward}')\n",
        "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "        \n",
        "        if timesteps_since_eval >= eval_freq:\n",
        "            timesteps_since_eval %=  eval_freq\n",
        "            evaluation.append(evaluate_policy(policy))\n",
        "            policy.save(file_name, directory='./pytorch_models')\n",
        "            np.save('./results/%s'%(file_name), evaluation)\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    #Before 10000 timesteps,we play Random actions.\n",
        "    if total_timesteps < start_timesteps:\n",
        "        action = env.action_space.sample()\n",
        "    else:#after 10000 we switch to the policy/model/agent\n",
        "        action = policy.select_action(np.array(obs))\n",
        "        if expi_noise != 0:\n",
        "            action = (action + np.random.normal(0, expi_noise, size=env.action_space.shape[0])).clip(env.action_space.low,env.action_space.high)\n",
        "\n",
        "    new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "    episode_reward += reward\n",
        "    \n",
        "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "    obs = new_obs\n",
        "    episode_timesteps += 1\n",
        "    total_timesteps += 1\n",
        "    timesteps_since_eval += 1\n",
        "\n",
        "evaluation.append(evaluate_policy(policy))\n",
        "if save_model:\n",
        "    policy.save('%s'% (file_name), directory='./pytorch_models')\n",
        "np.save(\"./results/%s\" % (file_name),evaluation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RAlOA781O35",
        "outputId": "ad7cd892-0625-4ce6-c777-89418416c6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total timesteps:1000 - Episode num:1 - Reward:-1375.3721982538289\n",
            "Total timesteps:2000 - Episode num:2 - Reward:-1163.1261925903684\n",
            "Total timesteps:3000 - Episode num:3 - Reward:-1387.7243022273033\n",
            "Total timesteps:4000 - Episode num:4 - Reward:-1401.8537855468549\n",
            "Total timesteps:5000 - Episode num:5 - Reward:-1369.348934152938\n",
            "Average award over 10 is: -1366.971556953496\n",
            "Total timesteps:6000 - Episode num:6 - Reward:-1130.0099525244614\n",
            "Total timesteps:7000 - Episode num:7 - Reward:-1209.1116151025442\n",
            "Total timesteps:8000 - Episode num:8 - Reward:-1385.8684609654324\n",
            "Total timesteps:9000 - Episode num:9 - Reward:-1102.6932799629658\n",
            "Total timesteps:10000 - Episode num:10 - Reward:-1301.3627464447172\n",
            "Average award over 10 is: -1713.0424745987561\n",
            "Total timesteps:11000 - Episode num:11 - Reward:-1703.8003326247376\n",
            "Total timesteps:12000 - Episode num:12 - Reward:-1669.3190665044506\n",
            "Total timesteps:13000 - Episode num:13 - Reward:-1687.152933193019\n",
            "Total timesteps:14000 - Episode num:14 - Reward:-1301.4297801512225\n",
            "Total timesteps:15000 - Episode num:15 - Reward:-1674.5884048773476\n",
            "Average award over 10 is: -1703.134185598099\n",
            "Total timesteps:16000 - Episode num:16 - Reward:-1702.5717855859696\n",
            "Total timesteps:17000 - Episode num:17 - Reward:-1507.8406968058505\n",
            "Total timesteps:18000 - Episode num:18 - Reward:-1702.2678248594111\n",
            "Total timesteps:19000 - Episode num:19 - Reward:-64.7025361565141\n",
            "Total timesteps:20000 - Episode num:20 - Reward:-1596.7137522522687\n",
            "Average award over 10 is: -1625.8766280253615\n",
            "Total timesteps:21000 - Episode num:21 - Reward:-1678.0447280490112\n",
            "Total timesteps:22000 - Episode num:22 - Reward:-1525.1225361547176\n",
            "Total timesteps:23000 - Episode num:23 - Reward:-1681.8101389041112\n",
            "Total timesteps:24000 - Episode num:24 - Reward:-1569.7755460943554\n",
            "Total timesteps:25000 - Episode num:25 - Reward:-1649.1769537863302\n",
            "Average award over 10 is: -1613.422237409437\n",
            "Total timesteps:26000 - Episode num:26 - Reward:-1586.6087531463995\n",
            "Total timesteps:27000 - Episode num:27 - Reward:-1616.7893086706124\n",
            "Total timesteps:28000 - Episode num:28 - Reward:-1450.8767725621115\n",
            "Total timesteps:29000 - Episode num:29 - Reward:-965.2230136774695\n",
            "Total timesteps:30000 - Episode num:30 - Reward:-1433.0037509198255\n",
            "Average award over 10 is: -759.9055125834441\n",
            "Total timesteps:31000 - Episode num:31 - Reward:225.70690720171893\n",
            "Total timesteps:32000 - Episode num:32 - Reward:-1552.0312811034928\n",
            "Total timesteps:33000 - Episode num:33 - Reward:-1229.669298531673\n",
            "Total timesteps:34000 - Episode num:34 - Reward:-1476.7898022325298\n",
            "Total timesteps:35000 - Episode num:35 - Reward:-1530.813703203957\n",
            "Average award over 10 is: -509.5047198539355\n",
            "Total timesteps:36000 - Episode num:36 - Reward:-544.7889135418554\n",
            "Total timesteps:37000 - Episode num:37 - Reward:-767.5424698123238\n",
            "Total timesteps:38000 - Episode num:38 - Reward:-1567.9037223461198\n",
            "Total timesteps:39000 - Episode num:39 - Reward:-1245.7989244671191\n",
            "Total timesteps:40000 - Episode num:40 - Reward:-1056.9986325140296\n",
            "Average award over 10 is: -1183.472470344724\n",
            "Total timesteps:41000 - Episode num:41 - Reward:-999.5093574313511\n",
            "Total timesteps:42000 - Episode num:42 - Reward:-1361.071254210259\n",
            "Total timesteps:43000 - Episode num:43 - Reward:-292.34694999128794\n",
            "Total timesteps:44000 - Episode num:44 - Reward:-506.1815244986246\n",
            "Total timesteps:45000 - Episode num:45 - Reward:-1434.4096678682274\n",
            "Average award over 10 is: -1427.4598352604594\n",
            "Total timesteps:46000 - Episode num:46 - Reward:-1339.4951468096303\n",
            "Total timesteps:47000 - Episode num:47 - Reward:-1074.606795396909\n",
            "Total timesteps:48000 - Episode num:48 - Reward:-1284.1428139962823\n",
            "Total timesteps:49000 - Episode num:49 - Reward:-414.60505713924135\n",
            "Total timesteps:50000 - Episode num:50 - Reward:-436.76918441907134\n",
            "Average award over 10 is: -340.43510983201554\n",
            "Total timesteps:51000 - Episode num:51 - Reward:442.8697318391386\n",
            "Total timesteps:52000 - Episode num:52 - Reward:-402.273516709959\n",
            "Total timesteps:53000 - Episode num:53 - Reward:-202.2814556345397\n",
            "Total timesteps:54000 - Episode num:54 - Reward:-447.8871364023579\n",
            "Total timesteps:55000 - Episode num:55 - Reward:-287.76513423604575\n",
            "Average award over 10 is: -694.040101656062\n",
            "Total timesteps:56000 - Episode num:56 - Reward:-571.7364152282437\n",
            "Total timesteps:57000 - Episode num:57 - Reward:433.1114120489893\n",
            "Total timesteps:58000 - Episode num:58 - Reward:416.42439452606374\n",
            "Total timesteps:59000 - Episode num:59 - Reward:-640.7601317227296\n",
            "Total timesteps:60000 - Episode num:60 - Reward:-968.9199365497429\n",
            "Average award over 10 is: 84.36223751899446\n",
            "Total timesteps:61000 - Episode num:61 - Reward:-539.6232876674427\n",
            "Total timesteps:62000 - Episode num:62 - Reward:-999.7853299919333\n",
            "Total timesteps:63000 - Episode num:63 - Reward:-1049.8879831341626\n",
            "Total timesteps:64000 - Episode num:64 - Reward:-974.5739400587446\n",
            "Total timesteps:65000 - Episode num:65 - Reward:-966.9947847219037\n",
            "Average award over 10 is: -1343.7568472706103\n",
            "Total timesteps:66000 - Episode num:66 - Reward:-1226.250638649136\n",
            "Total timesteps:67000 - Episode num:67 - Reward:-713.5103284176419\n",
            "Total timesteps:68000 - Episode num:68 - Reward:-1090.7013988703584\n",
            "Total timesteps:69000 - Episode num:69 - Reward:-489.2157548849022\n",
            "Total timesteps:70000 - Episode num:70 - Reward:-945.5587899998619\n",
            "Average award over 10 is: -874.1624982591578\n",
            "Total timesteps:71000 - Episode num:71 - Reward:-817.682886693491\n",
            "Total timesteps:72000 - Episode num:72 - Reward:-1407.3945631163238\n",
            "Total timesteps:73000 - Episode num:73 - Reward:48.3842013319824\n",
            "Total timesteps:74000 - Episode num:74 - Reward:-154.3037673133354\n",
            "Total timesteps:75000 - Episode num:75 - Reward:-257.7468298448527\n",
            "Average award over 10 is: -533.2630841939905\n",
            "Total timesteps:76000 - Episode num:76 - Reward:118.5092457201114\n",
            "Total timesteps:77000 - Episode num:77 - Reward:229.36612027394136\n",
            "Total timesteps:78000 - Episode num:78 - Reward:460.5276261504718\n",
            "Total timesteps:79000 - Episode num:79 - Reward:168.61556711073018\n",
            "Total timesteps:80000 - Episode num:80 - Reward:576.1686892840463\n",
            "Average award over 10 is: 430.41303442360214\n",
            "Total timesteps:81000 - Episode num:81 - Reward:584.0121771374132\n",
            "Total timesteps:82000 - Episode num:82 - Reward:508.19203661789453\n",
            "Total timesteps:83000 - Episode num:83 - Reward:634.1307148528505\n",
            "Total timesteps:84000 - Episode num:84 - Reward:632.8910381132436\n",
            "Total timesteps:85000 - Episode num:85 - Reward:644.2637963426041\n",
            "Average award over 10 is: 230.48740042586797\n",
            "Total timesteps:86000 - Episode num:86 - Reward:384.0680929864099\n",
            "Total timesteps:87000 - Episode num:87 - Reward:-32.51736997160925\n",
            "Total timesteps:88000 - Episode num:88 - Reward:507.1626742203444\n",
            "Total timesteps:89000 - Episode num:89 - Reward:444.32656264262124\n",
            "Total timesteps:90000 - Episode num:90 - Reward:138.25011051776355\n",
            "Average award over 10 is: 543.892678565335\n",
            "Total timesteps:91000 - Episode num:91 - Reward:436.35234837544226\n",
            "Total timesteps:92000 - Episode num:92 - Reward:501.74295362799717\n",
            "Total timesteps:93000 - Episode num:93 - Reward:572.8918324785351\n",
            "Total timesteps:94000 - Episode num:94 - Reward:418.62294918298863\n",
            "Total timesteps:95000 - Episode num:95 - Reward:701.4958785250498\n",
            "Average award over 10 is: 555.4430521194225\n",
            "Total timesteps:96000 - Episode num:96 - Reward:529.7269419453884\n",
            "Total timesteps:97000 - Episode num:97 - Reward:458.2787274518217\n",
            "Total timesteps:98000 - Episode num:98 - Reward:566.5357535225658\n",
            "Total timesteps:99000 - Episode num:99 - Reward:509.76901153313105\n",
            "Total timesteps:100000 - Episode num:100 - Reward:536.5364284512676\n",
            "Average award over 10 is: 623.8435972837135\n",
            "Total timesteps:101000 - Episode num:101 - Reward:621.4419948720742\n",
            "Total timesteps:102000 - Episode num:102 - Reward:470.9014991033278\n",
            "Total timesteps:103000 - Episode num:103 - Reward:463.5882864606488\n",
            "Total timesteps:104000 - Episode num:104 - Reward:630.6502550652467\n",
            "Total timesteps:105000 - Episode num:105 - Reward:556.7298396484892\n",
            "Average award over 10 is: 649.316788315776\n",
            "Total timesteps:106000 - Episode num:106 - Reward:653.7044514799942\n",
            "Total timesteps:107000 - Episode num:107 - Reward:245.4809760792971\n",
            "Total timesteps:108000 - Episode num:108 - Reward:311.3172095738058\n",
            "Total timesteps:109000 - Episode num:109 - Reward:559.8681145205384\n",
            "Total timesteps:110000 - Episode num:110 - Reward:695.3002933241601\n",
            "Average award over 10 is: 603.3389514263002\n",
            "Total timesteps:111000 - Episode num:111 - Reward:600.4109708646703\n",
            "Total timesteps:112000 - Episode num:112 - Reward:572.3979348315283\n",
            "Total timesteps:113000 - Episode num:113 - Reward:607.6134436756195\n",
            "Total timesteps:114000 - Episode num:114 - Reward:572.6026068359218\n",
            "Total timesteps:115000 - Episode num:115 - Reward:611.4791744980961\n",
            "Average award over 10 is: 682.9997485159092\n",
            "Total timesteps:116000 - Episode num:116 - Reward:672.0299002290942\n",
            "Total timesteps:117000 - Episode num:117 - Reward:624.0990729860765\n",
            "Total timesteps:118000 - Episode num:118 - Reward:-346.206591681032\n",
            "Total timesteps:119000 - Episode num:119 - Reward:600.6962494016659\n",
            "Total timesteps:120000 - Episode num:120 - Reward:600.5827920518351\n",
            "Average award over 10 is: 542.2784488320101\n",
            "Total timesteps:121000 - Episode num:121 - Reward:576.5179590001192\n",
            "Total timesteps:122000 - Episode num:122 - Reward:682.8803915256254\n",
            "Total timesteps:123000 - Episode num:123 - Reward:775.8129172957987\n",
            "Total timesteps:124000 - Episode num:124 - Reward:669.0078271376988\n",
            "Total timesteps:125000 - Episode num:125 - Reward:845.6448067902237\n",
            "Average award over 10 is: 669.5858745819334\n",
            "Total timesteps:126000 - Episode num:126 - Reward:566.9683738030482\n",
            "Total timesteps:127000 - Episode num:127 - Reward:612.6020482076221\n",
            "Total timesteps:128000 - Episode num:128 - Reward:719.741902516514\n",
            "Total timesteps:129000 - Episode num:129 - Reward:703.5032697098337\n",
            "Total timesteps:130000 - Episode num:130 - Reward:581.2538637217057\n",
            "Average award over 10 is: 551.2084314072432\n",
            "Total timesteps:131000 - Episode num:131 - Reward:479.39296596815996\n",
            "Total timesteps:132000 - Episode num:132 - Reward:616.5895647756986\n",
            "Total timesteps:133000 - Episode num:133 - Reward:611.1707553026754\n",
            "Total timesteps:134000 - Episode num:134 - Reward:658.5798549902115\n",
            "Total timesteps:135000 - Episode num:135 - Reward:155.56413023075348\n",
            "Average award over 10 is: 558.7797394780857\n",
            "Total timesteps:136000 - Episode num:136 - Reward:492.9372065409997\n",
            "Total timesteps:137000 - Episode num:137 - Reward:622.4027234523483\n",
            "Total timesteps:138000 - Episode num:138 - Reward:451.52923355930534\n",
            "Total timesteps:139000 - Episode num:139 - Reward:613.9225461129236\n",
            "Total timesteps:140000 - Episode num:140 - Reward:582.026443785859\n",
            "Average award over 10 is: 677.5736560462781\n",
            "Total timesteps:141000 - Episode num:141 - Reward:652.886326356971\n",
            "Total timesteps:142000 - Episode num:142 - Reward:491.6232927644616\n",
            "Total timesteps:143000 - Episode num:143 - Reward:666.3404311275995\n",
            "Total timesteps:144000 - Episode num:144 - Reward:635.4801016287644\n",
            "Total timesteps:145000 - Episode num:145 - Reward:615.5524021942701\n",
            "Average award over 10 is: 627.3298759434313\n",
            "Total timesteps:146000 - Episode num:146 - Reward:652.3582945955196\n",
            "Total timesteps:147000 - Episode num:147 - Reward:613.1090132951183\n",
            "Total timesteps:148000 - Episode num:148 - Reward:587.4049563956078\n",
            "Total timesteps:149000 - Episode num:149 - Reward:737.7241580373583\n",
            "Total timesteps:150000 - Episode num:150 - Reward:491.07382760741535\n",
            "Average award over 10 is: 534.851192103523\n",
            "Total timesteps:151000 - Episode num:151 - Reward:554.2721060170397\n",
            "Total timesteps:152000 - Episode num:152 - Reward:333.69127522472644\n",
            "Total timesteps:153000 - Episode num:153 - Reward:454.32553667135437\n",
            "Total timesteps:154000 - Episode num:154 - Reward:435.5655167909503\n",
            "Total timesteps:155000 - Episode num:155 - Reward:646.6522500526132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "fpYFDyi11O6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pybullet"
      ],
      "metadata": {
        "id": "aGIKRBgWaXe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "ZuFkSYESaWPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr+1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition) \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        sample_data = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "\n",
        "        states_ = []\n",
        "        next_states_ = [] \n",
        "        actions_ = []\n",
        "        rewards_ = []\n",
        "        dones_= []\n",
        "        for i in sample_data:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            states_.append(state)\n",
        "            next_states_.append(next_state)\n",
        "            actions_.append(action)\n",
        "            rewards_.append(reward)\n",
        "            dones_.append(done)\n",
        "        \n",
        "        return np.array(states_), np.array(next_states_), np.array(actions_), np.array(rewards_).reshape(-1,1), np.array(dones_).reshape(-1,1)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input, action, cut):#action is the number outputs | output is the number of actions\n",
        "        super(Actor,self).__init__()\n",
        "        self.fully01 = nn.Linear(input, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.last = nn.Linear(300, action)\n",
        "        self.cut = cut\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fully01(x))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        return self.cut * torch.tanh(self.last(x))#the cut to adjust to the output levels. higher or lower that -1,1                             \n",
        "\n",
        "#since we need two pair of critics, im making both on the same class.\n",
        "#the name of the class should be DoubleCritic,PairCritic..etc\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input, action):\n",
        "        super(Critic, self).__init__()\n",
        "        #first\n",
        "        self.fully01 = nn.Linear(input+action, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.fully03 = nn.Linear(300, 1)\n",
        "\n",
        "        #second\n",
        "        self.fully11 = nn.Linear(input+action, 400)\n",
        "        self.fully22 = nn.Linear(400,300)\n",
        "        self.fully33 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        #first\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        #Second\n",
        "        y = F.relu(self.fully11(xu))\n",
        "        y = F.relu(self.fully22(y))\n",
        "        y = self.fully33(y)\n",
        "        return x, y\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        return x\n",
        "\n",
        "#training\n",
        "class TD3(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        # actors\n",
        "        self.Actor = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target.load_state_dict(self.Actor.state_dict())\n",
        "\n",
        "        # actor optimizer\n",
        "        self.Actor_optimizer = torch.optim.Adam(self.Actor.parameters())\n",
        "\n",
        "        ## Critic \n",
        "        self.Critic = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target.load_state_dict(self.Critic.state_dict())\n",
        "\n",
        "        ## Critic optimizer\n",
        "        self.Critic_optimizer = torch.optim.Adam(self.Critic.parameters())\n",
        "\n",
        "        ### Max_Action is the cut/clip\n",
        "        self.max_action =  max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1,-1)).to(DEVICE)\n",
        "        return self.Actor(state).cpu().data.numpy().flatten()\n",
        "        #return self.Actor(state).data.numpy().flatten()\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.Actor.state_dict(),'%s/%s_Actor.pth' % (directory,filename))\n",
        "        torch.save(self.Critic.state_dict(),'%s/%s_Critic.pth' % (directory,filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.Actor.load_state_dict(torch.load('%s/%s_Actor.pth' % (directory,filename)))\n",
        "        self.Critic.load_state_dict(torch.load('%s/%s_Critic.pth' % (directory,filename)))\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for i in range(iterations):\n",
        "            states_, next_states_, actions_, rewards_, dones_ = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.Tensor(states_).to(DEVICE)\n",
        "            next_state = torch.Tensor(next_states_).to(DEVICE)\n",
        "            action = torch.Tensor(actions_).to(DEVICE)\n",
        "            reward = torch.Tensor(rewards_).to(DEVICE)\n",
        "            done = torch.Tensor(dones_).to(DEVICE)\n",
        "\n",
        "            next_action = self.Actor_Target(next_state)\n",
        "\n",
        "            noise = torch.Tensor(actions_).data.normal_(0, policy_noise).to(DEVICE)\n",
        "            noise = noise.clamp(-noise_clip,noise_clip)\n",
        "            next_action = (next_action+noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            Target_Q1, Target_Q2 = self.Critic_Target(next_state, next_action)\n",
        "            \n",
        "            #when episode is over 1, not over 0. \n",
        "            # we detached because adding the reward which is the output \n",
        "            #of nn to the computaional graph would not be what we want.\n",
        "            Target_Q = torch.min(Target_Q1, Target_Q2)\n",
        "            Target_Q = reward + (discount * Target_Q * (1 - done)).detach()\n",
        "          \n",
        "            Current_Q1, Current_Q2 = self.Critic(state, action)\n",
        "            critic_loss = F.mse_loss(Current_Q1,Target_Q) + F.mse_loss(Current_Q2, Target_Q)\n",
        "\n",
        "            self.Critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.Critic_optimizer.step()\n",
        "\n",
        "            if not i % policy_freq:\n",
        "                actor_loss = -self.Critic.Q1(state, self.Actor(state)).mean()\n",
        "                self.Actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.Actor_optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.Actor.parameters(), self.Actor_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "                \n",
        "                for param, target_param in zip(self.Critic.parameters(), self.Critic_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "def evaluate_policy(policy, episodes=10):\n",
        "    avg_awards = 0\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_awards += reward\n",
        "    avg_awards /= episodes\n",
        "    print(f\"Average award over {episodes} is:\",avg_awards)\n",
        "    return avg_awards\n",
        "\n",
        "def mkdir(base, name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "metadata": {
        "id": "RFvqs75A1O8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'HalfCheetahBulletEnv-v0'\n",
        "seed = 0\n",
        "file_name = f\"TD3--{env_name}--seed({seed})\"\n",
        "print(file_name)\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "\n",
        "max_episode_step = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "    env.reset()\n",
        "#setting env\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "#agent\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, \"./pytorch_models/\")\n",
        "_ = evaluate_policy(policy, episodes=eval_episodes)"
      ],
      "metadata": {
        "id": "UxWlrx49KRPI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}