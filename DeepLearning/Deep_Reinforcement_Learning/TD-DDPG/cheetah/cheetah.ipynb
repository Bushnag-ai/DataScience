{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pybullet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB6SkSb78G6_",
        "outputId": "3e9cb632-100c-42ad-9125-e06cef812c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (90.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 90.8 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "QBGrjvQs5ZoU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU on:', True if torch.cuda.is_available() else False, '| Device:',DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRTSDnr3WCm9",
        "outputId": "c7f52b4e-8525-4f8f-9a7d-ecd706e9e219"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU on: True | Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr+1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition) \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        sample_data = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "\n",
        "        states_ = []\n",
        "        next_states_ = [] \n",
        "        actions_ = []\n",
        "        rewards_ = []\n",
        "        dones_= []\n",
        "        for i in sample_data:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            states_.append(state)\n",
        "            next_states_.append(next_state)\n",
        "            actions_.append(action)\n",
        "            rewards_.append(reward)\n",
        "            dones_.append(done)\n",
        "        \n",
        "        return np.array(states_), np.array(next_states_), np.array(actions_), np.array(rewards_).reshape(-1,1), np.array(dones_).reshape(-1,1)"
      ],
      "metadata": {
        "id": "MAzkCTVL8Cz6"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input, action, cut):#action is the number outputs | output is the number of actions\n",
        "        super(Actor,self).__init__()\n",
        "        self.fully01 = nn.Linear(input, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.last = nn.Linear(300, action)\n",
        "        self.cut = cut\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fully01(x))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        return self.cut * torch.tanh(self.last(x))#the cut to adjust to the output levels. higher or lower that -1,1                             "
      ],
      "metadata": {
        "id": "zaWQLv_E-19F"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since we need two pair of critics, im making both on the same class.\n",
        "#the name of the class should be DoubleCritic,PairCritic..etc\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input, action):\n",
        "        super(Critic, self).__init__()\n",
        "        #first\n",
        "        self.fully01 = nn.Linear(input+action, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.fully03 = nn.Linear(300, 1)\n",
        "\n",
        "        #second\n",
        "        self.fully11 = nn.Linear(input+action, 400)\n",
        "        self.fully22 = nn.Linear(400,300)\n",
        "        self.fully33 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        #first\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        #Second\n",
        "        y = F.relu(self.fully11(xu))\n",
        "        y = F.relu(self.fully22(y))\n",
        "        y = self.fully33(y)\n",
        "        return x, y\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NgrMw3mE-24P"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "class TD3(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        # actors\n",
        "        self.Actor = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target.load_state_dict(self.Actor.state_dict())\n",
        "\n",
        "        # actor optimizer\n",
        "        self.Actor_optimizer = torch.optim.Adam(self.Actor.parameters())\n",
        "\n",
        "        ## Critic \n",
        "        self.Critic = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target.load_state_dict(self.Critic.state_dict())\n",
        "\n",
        "        ## Critic optimizer\n",
        "        self.Critic_optimizer = torch.optim.Adam(self.Critic.parameters())\n",
        "\n",
        "        ### Max_Action is the cut/clip\n",
        "        self.max_action =  max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1,-1)).to(DEVICE)\n",
        "        return self.Actor(state).cpu().data.numpy().flatten()\n",
        "        #return self.Actor(state).data.numpy().flatten()\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.Actor.state_dict(),'%s/%s_Actor.pth' % (directory,filename))\n",
        "        torch.save(self.Critic.state_dict(),'%s/%s_Critic.pth' % (directory,filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.Actor.load_state_dict(torch.load('%s/%s_Actor.pth' % (directory,filename)))\n",
        "        self.Critic.load_state_dict(torch.load('%s/%s_Critic.pth' % (directory,filename)))\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for i in range(iterations):\n",
        "            states_, next_states_, actions_, rewards_, dones_ = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.Tensor(states_).to(DEVICE)\n",
        "            next_state = torch.Tensor(next_states_).to(DEVICE)\n",
        "            action = torch.Tensor(actions_).to(DEVICE)\n",
        "            reward = torch.Tensor(rewards_).to(DEVICE)\n",
        "            done = torch.Tensor(dones_).to(DEVICE)\n",
        "\n",
        "            next_action = self.Actor_Target(next_state)\n",
        "\n",
        "            noise = torch.Tensor(actions_).data.normal_(0, policy_noise).to(DEVICE)\n",
        "            noise = noise.clamp(-noise_clip,noise_clip)\n",
        "            next_action = (next_action+noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            Target_Q1, Target_Q2 = self.Critic_Target(next_state, next_action)\n",
        "            \n",
        "            #when episode is over 1, not over 0. \n",
        "            # we detached because adding the reward which is the output \n",
        "            #of nn to the computaional graph would not be what we want.\n",
        "            Target_Q = torch.min(Target_Q1, Target_Q2)\n",
        "            Target_Q = reward + (discount * Target_Q * (1 - done)).detach()\n",
        "          \n",
        "            Current_Q1, Current_Q2 = self.Critic(state, action)\n",
        "            critic_loss = F.mse_loss(Current_Q1,Target_Q) + F.mse_loss(Current_Q2, Target_Q)\n",
        "\n",
        "            self.Critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.Critic_optimizer.step()\n",
        "\n",
        "            if not i % policy_freq:\n",
        "                actor_loss = -self.Critic.Q1(state, self.Actor(state)).mean()\n",
        "                self.Actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.Actor_optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.Actor.parameters(), self.Actor_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "                \n",
        "                for param, target_param in zip(self.Critic.parameters(), self.Critic_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)"
      ],
      "metadata": {
        "id": "YvN3Pdw2VKXr"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy, episodes=10):\n",
        "    avg_awards = 0\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_awards += reward\n",
        "    avg_awards /= episodes\n",
        "    print(f\"Average award over {episodes} is:\",avg_awards)\n",
        "    return avg_awards"
      ],
      "metadata": {
        "id": "uRc2bSmh91_o"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "env_name = 'HalfCheetahBulletEnv-v0'\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps= 6e5\n",
        "\n",
        "save_model = True\n",
        "expi_noise = 0.1\n",
        "batch_size = 100\n",
        "discount = 0.99\n",
        "tau = 0.005\n",
        "\n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5\n",
        "policy_freq = 2"
      ],
      "metadata": {
        "id": "syR2bCx7VKce"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "U6NG65vZVKej"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = f\"TD3--{env_name}--seed({seed})\"\n",
        "print(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iPqcoSTvxYR",
        "outputId": "8688082e-27ce-4ec5-8626-f7462ea98434"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD3--HalfCheetahBulletEnv-v0--seed(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./results'):\n",
        "    os.makedirs('./results')\n",
        "if save_model and not os.path.exists('./pytorch_models'):\n",
        "    os.makedirs('./pytorch_models')"
      ],
      "metadata": {
        "id": "lULFXN0t0bci"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting env\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "metadata": {
        "id": "9u7zaZkd1Oqf"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "metadata": {
        "id": "hafzat8h1Osy"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = ReplayBuffer() "
      ],
      "metadata": {
        "id": "KQv8Y-Q_1Ou4"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = [evaluate_policy(policy)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug1NGAnn1OxH",
        "outputId": "a7213447-9af3-41d8-e5c9-5c8fcf7d9dc9"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average award over 10 is: -1429.4266421472653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "    \n",
        "work_dir = mkdir('exp','brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_step = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "G1HcVeyp1OzO"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing the variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "VCpTqgBc1O1v"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while total_timesteps < max_timesteps:\n",
        "    if done:\n",
        "        if total_timesteps != 0:\n",
        "            print(f'Total timesteps:{total_timesteps} - Episode num:{episode_num} - Reward:{episode_reward}')\n",
        "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "        \n",
        "        if timesteps_since_eval >= eval_freq:\n",
        "            timesteps_since_eval %=  eval_freq\n",
        "            evaluation.append(evaluate_policy(policy))\n",
        "            policy.save(file_name, directory='./pytorch_models')\n",
        "            np.save('./results/%s'%(file_name), evaluation)\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    #Before 10000 timesteps,we play Random actions.\n",
        "    if total_timesteps < start_timesteps:\n",
        "        action = env.action_space.sample()\n",
        "    else:#after 10000 we switch to the policy/model/agent\n",
        "        action = policy.select_action(np.array(obs))\n",
        "        if expi_noise != 0:\n",
        "            action = (action + np.random.normal(0, expi_noise, size=env.action_space.shape[0])).clip(env.action_space.low,env.action_space.high)\n",
        "\n",
        "    new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "    episode_reward += reward\n",
        "    \n",
        "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "    obs = new_obs\n",
        "    episode_timesteps += 1\n",
        "    total_timesteps += 1\n",
        "    timesteps_since_eval += 1\n",
        "\n",
        "evaluation.append(evaluate_policy(policy))\n",
        "if save_model:\n",
        "    policy.save('%s'% (file_name), directory='./pytorch_models')\n",
        "np.save(\"./results/%s\" % (file_name),evaluation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RAlOA781O35",
        "outputId": "ad7cd892-0625-4ce6-c777-89418416c6d9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total timesteps:1000 - Episode num:1 - Reward:-1375.3721982538289\n",
            "Total timesteps:2000 - Episode num:2 - Reward:-1163.1261925903684\n",
            "Total timesteps:3000 - Episode num:3 - Reward:-1387.7243022273033\n",
            "Total timesteps:4000 - Episode num:4 - Reward:-1401.8537855468549\n",
            "Total timesteps:5000 - Episode num:5 - Reward:-1369.348934152938\n",
            "Average award over 10 is: -1366.971556953496\n",
            "Total timesteps:6000 - Episode num:6 - Reward:-1130.0099525244614\n",
            "Total timesteps:7000 - Episode num:7 - Reward:-1209.1116151025442\n",
            "Total timesteps:8000 - Episode num:8 - Reward:-1385.8684609654324\n",
            "Total timesteps:9000 - Episode num:9 - Reward:-1102.6932799629658\n",
            "Total timesteps:10000 - Episode num:10 - Reward:-1301.3627464447172\n",
            "Average award over 10 is: -1713.0424745987561\n",
            "Total timesteps:11000 - Episode num:11 - Reward:-1703.8003326247376\n",
            "Total timesteps:12000 - Episode num:12 - Reward:-1669.3190665044506\n",
            "Total timesteps:13000 - Episode num:13 - Reward:-1687.152933193019\n",
            "Total timesteps:14000 - Episode num:14 - Reward:-1301.4297801512225\n",
            "Total timesteps:15000 - Episode num:15 - Reward:-1674.5884048773476\n",
            "Average award over 10 is: -1703.134185598099\n",
            "Total timesteps:16000 - Episode num:16 - Reward:-1702.5717855859696\n",
            "Total timesteps:17000 - Episode num:17 - Reward:-1507.8406968058505\n",
            "Total timesteps:18000 - Episode num:18 - Reward:-1702.2678248594111\n",
            "Total timesteps:19000 - Episode num:19 - Reward:-64.7025361565141\n",
            "Total timesteps:20000 - Episode num:20 - Reward:-1596.7137522522687\n",
            "Average award over 10 is: -1625.8766280253615\n",
            "Total timesteps:21000 - Episode num:21 - Reward:-1678.0447280490112\n",
            "Total timesteps:22000 - Episode num:22 - Reward:-1525.1225361547176\n",
            "Total timesteps:23000 - Episode num:23 - Reward:-1681.8101389041112\n",
            "Total timesteps:24000 - Episode num:24 - Reward:-1569.7755460943554\n",
            "Total timesteps:25000 - Episode num:25 - Reward:-1649.1769537863302\n",
            "Average award over 10 is: -1613.422237409437\n",
            "Total timesteps:26000 - Episode num:26 - Reward:-1586.6087531463995\n",
            "Total timesteps:27000 - Episode num:27 - Reward:-1616.7893086706124\n",
            "Total timesteps:28000 - Episode num:28 - Reward:-1450.8767725621115\n",
            "Total timesteps:29000 - Episode num:29 - Reward:-965.2230136774695\n",
            "Total timesteps:30000 - Episode num:30 - Reward:-1433.0037509198255\n",
            "Average award over 10 is: -759.9055125834441\n",
            "Total timesteps:31000 - Episode num:31 - Reward:225.70690720171893\n",
            "Total timesteps:32000 - Episode num:32 - Reward:-1552.0312811034928\n",
            "Total timesteps:33000 - Episode num:33 - Reward:-1229.669298531673\n",
            "Total timesteps:34000 - Episode num:34 - Reward:-1476.7898022325298\n",
            "Total timesteps:35000 - Episode num:35 - Reward:-1530.813703203957\n",
            "Average award over 10 is: -509.5047198539355\n",
            "Total timesteps:36000 - Episode num:36 - Reward:-544.7889135418554\n",
            "Total timesteps:37000 - Episode num:37 - Reward:-767.5424698123238\n",
            "Total timesteps:38000 - Episode num:38 - Reward:-1567.9037223461198\n",
            "Total timesteps:39000 - Episode num:39 - Reward:-1245.7989244671191\n",
            "Total timesteps:40000 - Episode num:40 - Reward:-1056.9986325140296\n",
            "Average award over 10 is: -1183.472470344724\n",
            "Total timesteps:41000 - Episode num:41 - Reward:-999.5093574313511\n",
            "Total timesteps:42000 - Episode num:42 - Reward:-1361.071254210259\n",
            "Total timesteps:43000 - Episode num:43 - Reward:-292.34694999128794\n",
            "Total timesteps:44000 - Episode num:44 - Reward:-506.1815244986246\n",
            "Total timesteps:45000 - Episode num:45 - Reward:-1434.4096678682274\n",
            "Average award over 10 is: -1427.4598352604594\n",
            "Total timesteps:46000 - Episode num:46 - Reward:-1339.4951468096303\n",
            "Total timesteps:47000 - Episode num:47 - Reward:-1074.606795396909\n",
            "Total timesteps:48000 - Episode num:48 - Reward:-1284.1428139962823\n",
            "Total timesteps:49000 - Episode num:49 - Reward:-414.60505713924135\n",
            "Total timesteps:50000 - Episode num:50 - Reward:-436.76918441907134\n",
            "Average award over 10 is: -340.43510983201554\n",
            "Total timesteps:51000 - Episode num:51 - Reward:442.8697318391386\n",
            "Total timesteps:52000 - Episode num:52 - Reward:-402.273516709959\n",
            "Total timesteps:53000 - Episode num:53 - Reward:-202.2814556345397\n",
            "Total timesteps:54000 - Episode num:54 - Reward:-447.8871364023579\n",
            "Total timesteps:55000 - Episode num:55 - Reward:-287.76513423604575\n",
            "Average award over 10 is: -694.040101656062\n",
            "Total timesteps:56000 - Episode num:56 - Reward:-571.7364152282437\n",
            "Total timesteps:57000 - Episode num:57 - Reward:433.1114120489893\n",
            "Total timesteps:58000 - Episode num:58 - Reward:416.42439452606374\n",
            "Total timesteps:59000 - Episode num:59 - Reward:-640.7601317227296\n",
            "Total timesteps:60000 - Episode num:60 - Reward:-968.9199365497429\n",
            "Average award over 10 is: 84.36223751899446\n",
            "Total timesteps:61000 - Episode num:61 - Reward:-539.6232876674427\n",
            "Total timesteps:62000 - Episode num:62 - Reward:-999.7853299919333\n",
            "Total timesteps:63000 - Episode num:63 - Reward:-1049.8879831341626\n",
            "Total timesteps:64000 - Episode num:64 - Reward:-974.5739400587446\n",
            "Total timesteps:65000 - Episode num:65 - Reward:-966.9947847219037\n",
            "Average award over 10 is: -1343.7568472706103\n",
            "Total timesteps:66000 - Episode num:66 - Reward:-1226.250638649136\n",
            "Total timesteps:67000 - Episode num:67 - Reward:-713.5103284176419\n",
            "Total timesteps:68000 - Episode num:68 - Reward:-1090.7013988703584\n",
            "Total timesteps:69000 - Episode num:69 - Reward:-489.2157548849022\n",
            "Total timesteps:70000 - Episode num:70 - Reward:-945.5587899998619\n",
            "Average award over 10 is: -874.1624982591578\n",
            "Total timesteps:71000 - Episode num:71 - Reward:-817.682886693491\n",
            "Total timesteps:72000 - Episode num:72 - Reward:-1407.3945631163238\n",
            "Total timesteps:73000 - Episode num:73 - Reward:48.3842013319824\n",
            "Total timesteps:74000 - Episode num:74 - Reward:-154.3037673133354\n",
            "Total timesteps:75000 - Episode num:75 - Reward:-257.7468298448527\n",
            "Average award over 10 is: -533.2630841939905\n",
            "Total timesteps:76000 - Episode num:76 - Reward:118.5092457201114\n",
            "Total timesteps:77000 - Episode num:77 - Reward:229.36612027394136\n",
            "Total timesteps:78000 - Episode num:78 - Reward:460.5276261504718\n",
            "Total timesteps:79000 - Episode num:79 - Reward:168.61556711073018\n",
            "Total timesteps:80000 - Episode num:80 - Reward:576.1686892840463\n",
            "Average award over 10 is: 430.41303442360214\n",
            "Total timesteps:81000 - Episode num:81 - Reward:584.0121771374132\n",
            "Total timesteps:82000 - Episode num:82 - Reward:508.19203661789453\n",
            "Total timesteps:83000 - Episode num:83 - Reward:634.1307148528505\n",
            "Total timesteps:84000 - Episode num:84 - Reward:632.8910381132436\n",
            "Total timesteps:85000 - Episode num:85 - Reward:644.2637963426041\n",
            "Average award over 10 is: 230.48740042586797\n",
            "Total timesteps:86000 - Episode num:86 - Reward:384.0680929864099\n",
            "Total timesteps:87000 - Episode num:87 - Reward:-32.51736997160925\n",
            "Total timesteps:88000 - Episode num:88 - Reward:507.1626742203444\n",
            "Total timesteps:89000 - Episode num:89 - Reward:444.32656264262124\n",
            "Total timesteps:90000 - Episode num:90 - Reward:138.25011051776355\n",
            "Average award over 10 is: 543.892678565335\n",
            "Total timesteps:91000 - Episode num:91 - Reward:436.35234837544226\n",
            "Total timesteps:92000 - Episode num:92 - Reward:501.74295362799717\n",
            "Total timesteps:93000 - Episode num:93 - Reward:572.8918324785351\n",
            "Total timesteps:94000 - Episode num:94 - Reward:418.62294918298863\n",
            "Total timesteps:95000 - Episode num:95 - Reward:701.4958785250498\n",
            "Average award over 10 is: 555.4430521194225\n",
            "Total timesteps:96000 - Episode num:96 - Reward:529.7269419453884\n",
            "Total timesteps:97000 - Episode num:97 - Reward:458.2787274518217\n",
            "Total timesteps:98000 - Episode num:98 - Reward:566.5357535225658\n",
            "Total timesteps:99000 - Episode num:99 - Reward:509.76901153313105\n",
            "Total timesteps:100000 - Episode num:100 - Reward:536.5364284512676\n",
            "Average award over 10 is: 623.8435972837135\n",
            "Total timesteps:101000 - Episode num:101 - Reward:621.4419948720742\n",
            "Total timesteps:102000 - Episode num:102 - Reward:470.9014991033278\n",
            "Total timesteps:103000 - Episode num:103 - Reward:463.5882864606488\n",
            "Total timesteps:104000 - Episode num:104 - Reward:630.6502550652467\n",
            "Total timesteps:105000 - Episode num:105 - Reward:556.7298396484892\n",
            "Average award over 10 is: 649.316788315776\n",
            "Total timesteps:106000 - Episode num:106 - Reward:653.7044514799942\n",
            "Total timesteps:107000 - Episode num:107 - Reward:245.4809760792971\n",
            "Total timesteps:108000 - Episode num:108 - Reward:311.3172095738058\n",
            "Total timesteps:109000 - Episode num:109 - Reward:559.8681145205384\n",
            "Total timesteps:110000 - Episode num:110 - Reward:695.3002933241601\n",
            "Average award over 10 is: 603.3389514263002\n",
            "Total timesteps:111000 - Episode num:111 - Reward:600.4109708646703\n",
            "Total timesteps:112000 - Episode num:112 - Reward:572.3979348315283\n",
            "Total timesteps:113000 - Episode num:113 - Reward:607.6134436756195\n",
            "Total timesteps:114000 - Episode num:114 - Reward:572.6026068359218\n",
            "Total timesteps:115000 - Episode num:115 - Reward:611.4791744980961\n",
            "Average award over 10 is: 682.9997485159092\n",
            "Total timesteps:116000 - Episode num:116 - Reward:672.0299002290942\n",
            "Total timesteps:117000 - Episode num:117 - Reward:624.0990729860765\n",
            "Total timesteps:118000 - Episode num:118 - Reward:-346.206591681032\n",
            "Total timesteps:119000 - Episode num:119 - Reward:600.6962494016659\n",
            "Total timesteps:120000 - Episode num:120 - Reward:600.5827920518351\n",
            "Average award over 10 is: 542.2784488320101\n",
            "Total timesteps:121000 - Episode num:121 - Reward:576.5179590001192\n",
            "Total timesteps:122000 - Episode num:122 - Reward:682.8803915256254\n",
            "Total timesteps:123000 - Episode num:123 - Reward:775.8129172957987\n",
            "Total timesteps:124000 - Episode num:124 - Reward:669.0078271376988\n",
            "Total timesteps:125000 - Episode num:125 - Reward:845.6448067902237\n",
            "Average award over 10 is: 669.5858745819334\n",
            "Total timesteps:126000 - Episode num:126 - Reward:566.9683738030482\n",
            "Total timesteps:127000 - Episode num:127 - Reward:612.6020482076221\n",
            "Total timesteps:128000 - Episode num:128 - Reward:719.741902516514\n",
            "Total timesteps:129000 - Episode num:129 - Reward:703.5032697098337\n",
            "Total timesteps:130000 - Episode num:130 - Reward:581.2538637217057\n",
            "Average award over 10 is: 551.2084314072432\n",
            "Total timesteps:131000 - Episode num:131 - Reward:479.39296596815996\n",
            "Total timesteps:132000 - Episode num:132 - Reward:616.5895647756986\n",
            "Total timesteps:133000 - Episode num:133 - Reward:611.1707553026754\n",
            "Total timesteps:134000 - Episode num:134 - Reward:658.5798549902115\n",
            "Total timesteps:135000 - Episode num:135 - Reward:155.56413023075348\n",
            "Average award over 10 is: 558.7797394780857\n",
            "Total timesteps:136000 - Episode num:136 - Reward:492.9372065409997\n",
            "Total timesteps:137000 - Episode num:137 - Reward:622.4027234523483\n",
            "Total timesteps:138000 - Episode num:138 - Reward:451.52923355930534\n",
            "Total timesteps:139000 - Episode num:139 - Reward:613.9225461129236\n",
            "Total timesteps:140000 - Episode num:140 - Reward:582.026443785859\n",
            "Average award over 10 is: 677.5736560462781\n",
            "Total timesteps:141000 - Episode num:141 - Reward:652.886326356971\n",
            "Total timesteps:142000 - Episode num:142 - Reward:491.6232927644616\n",
            "Total timesteps:143000 - Episode num:143 - Reward:666.3404311275995\n",
            "Total timesteps:144000 - Episode num:144 - Reward:635.4801016287644\n",
            "Total timesteps:145000 - Episode num:145 - Reward:615.5524021942701\n",
            "Average award over 10 is: 627.3298759434313\n",
            "Total timesteps:146000 - Episode num:146 - Reward:652.3582945955196\n",
            "Total timesteps:147000 - Episode num:147 - Reward:613.1090132951183\n",
            "Total timesteps:148000 - Episode num:148 - Reward:587.4049563956078\n",
            "Total timesteps:149000 - Episode num:149 - Reward:737.7241580373583\n",
            "Total timesteps:150000 - Episode num:150 - Reward:491.07382760741535\n",
            "Average award over 10 is: 534.851192103523\n",
            "Total timesteps:151000 - Episode num:151 - Reward:554.2721060170397\n",
            "Total timesteps:152000 - Episode num:152 - Reward:333.69127522472644\n",
            "Total timesteps:153000 - Episode num:153 - Reward:454.32553667135437\n",
            "Total timesteps:154000 - Episode num:154 - Reward:435.5655167909503\n",
            "Total timesteps:155000 - Episode num:155 - Reward:646.6522500526132\n",
            "Average award over 10 is: 806.8387398703277\n",
            "Total timesteps:156000 - Episode num:156 - Reward:825.7152280361206\n",
            "Total timesteps:157000 - Episode num:157 - Reward:799.4596183435914\n",
            "Total timesteps:158000 - Episode num:158 - Reward:651.3215726936293\n",
            "Total timesteps:159000 - Episode num:159 - Reward:851.1853583346635\n",
            "Total timesteps:160000 - Episode num:160 - Reward:665.711456661452\n",
            "Average award over 10 is: 501.14186416864703\n",
            "Total timesteps:161000 - Episode num:161 - Reward:992.2973077896394\n",
            "Total timesteps:162000 - Episode num:162 - Reward:658.2815330656309\n",
            "Total timesteps:163000 - Episode num:163 - Reward:676.1803558367295\n",
            "Total timesteps:164000 - Episode num:164 - Reward:479.3801520731951\n",
            "Total timesteps:165000 - Episode num:165 - Reward:660.9265787167335\n",
            "Average award over 10 is: 54.763118840568154\n",
            "Total timesteps:166000 - Episode num:166 - Reward:1008.6073024305799\n",
            "Total timesteps:167000 - Episode num:167 - Reward:1175.4977779072162\n",
            "Total timesteps:168000 - Episode num:168 - Reward:524.8463490011796\n",
            "Total timesteps:169000 - Episode num:169 - Reward:1098.9702764797216\n",
            "Total timesteps:170000 - Episode num:170 - Reward:1466.9293622429034\n",
            "Average award over 10 is: 1254.5874284567885\n",
            "Total timesteps:171000 - Episode num:171 - Reward:1154.2095472415942\n",
            "Total timesteps:172000 - Episode num:172 - Reward:1149.2881523635426\n",
            "Total timesteps:173000 - Episode num:173 - Reward:617.9237352358986\n",
            "Total timesteps:174000 - Episode num:174 - Reward:1289.8931335732211\n",
            "Total timesteps:175000 - Episode num:175 - Reward:1185.0205133817121\n",
            "Average award over 10 is: 1155.9508423598522\n",
            "Total timesteps:176000 - Episode num:176 - Reward:1167.0897305273743\n",
            "Total timesteps:177000 - Episode num:177 - Reward:1306.7114109520246\n",
            "Total timesteps:178000 - Episode num:178 - Reward:1278.1846321176797\n",
            "Total timesteps:179000 - Episode num:179 - Reward:1255.7820371988887\n",
            "Total timesteps:180000 - Episode num:180 - Reward:1474.880903106537\n",
            "Average award over 10 is: 1441.4619681585787\n",
            "Total timesteps:181000 - Episode num:181 - Reward:1407.4754232358428\n",
            "Total timesteps:182000 - Episode num:182 - Reward:1360.5218489669128\n",
            "Total timesteps:183000 - Episode num:183 - Reward:1071.0065837638372\n",
            "Total timesteps:184000 - Episode num:184 - Reward:1312.881696912628\n",
            "Total timesteps:185000 - Episode num:185 - Reward:1154.1480104144227\n",
            "Average award over 10 is: 1298.4442034510425\n",
            "Total timesteps:186000 - Episode num:186 - Reward:1074.2157948589713\n",
            "Total timesteps:187000 - Episode num:187 - Reward:1313.4112309821724\n",
            "Total timesteps:188000 - Episode num:188 - Reward:1410.1603632067918\n",
            "Total timesteps:189000 - Episode num:189 - Reward:1491.1395431084145\n",
            "Total timesteps:190000 - Episode num:190 - Reward:1252.7001622442301\n",
            "Average award over 10 is: 1552.6063586467044\n",
            "Total timesteps:191000 - Episode num:191 - Reward:1526.128779019036\n",
            "Total timesteps:192000 - Episode num:192 - Reward:1587.9919144664275\n",
            "Total timesteps:193000 - Episode num:193 - Reward:1455.3410624689702\n",
            "Total timesteps:194000 - Episode num:194 - Reward:1503.0135862195323\n",
            "Total timesteps:195000 - Episode num:195 - Reward:1293.5722077230403\n",
            "Average award over 10 is: 1547.720941381656\n",
            "Total timesteps:196000 - Episode num:196 - Reward:1565.1393376405563\n",
            "Total timesteps:197000 - Episode num:197 - Reward:1534.4919808988943\n",
            "Total timesteps:198000 - Episode num:198 - Reward:1735.4915634455033\n",
            "Total timesteps:199000 - Episode num:199 - Reward:1502.4015548831535\n",
            "Total timesteps:200000 - Episode num:200 - Reward:1431.9765296098922\n",
            "Average award over 10 is: 1429.746076663478\n",
            "Total timesteps:201000 - Episode num:201 - Reward:1554.9582144764313\n",
            "Total timesteps:202000 - Episode num:202 - Reward:1393.5265126111306\n",
            "Total timesteps:203000 - Episode num:203 - Reward:1581.8492892755767\n",
            "Total timesteps:204000 - Episode num:204 - Reward:1359.440550791775\n",
            "Total timesteps:205000 - Episode num:205 - Reward:831.5348631109819\n",
            "Average award over 10 is: 1517.6038925773873\n",
            "Total timesteps:206000 - Episode num:206 - Reward:1518.459374767889\n",
            "Total timesteps:207000 - Episode num:207 - Reward:1429.2793172304514\n",
            "Total timesteps:208000 - Episode num:208 - Reward:1485.7606025998066\n",
            "Total timesteps:209000 - Episode num:209 - Reward:1279.5994871411792\n",
            "Total timesteps:210000 - Episode num:210 - Reward:1447.960362675884\n",
            "Average award over 10 is: 1193.6163898252296\n",
            "Total timesteps:211000 - Episode num:211 - Reward:1238.500542410601\n",
            "Total timesteps:212000 - Episode num:212 - Reward:1355.8482057320866\n",
            "Total timesteps:213000 - Episode num:213 - Reward:1471.3081366245608\n",
            "Total timesteps:214000 - Episode num:214 - Reward:1427.4894095264704\n",
            "Total timesteps:215000 - Episode num:215 - Reward:1426.4668451892028\n",
            "Average award over 10 is: 1577.2032377589662\n",
            "Total timesteps:216000 - Episode num:216 - Reward:1560.7409690143668\n",
            "Total timesteps:217000 - Episode num:217 - Reward:1437.6788545073537\n",
            "Total timesteps:218000 - Episode num:218 - Reward:1131.5317832615765\n",
            "Total timesteps:219000 - Episode num:219 - Reward:1735.4345588764188\n",
            "Total timesteps:220000 - Episode num:220 - Reward:1578.4078561884312\n",
            "Average award over 10 is: 1660.4329150937126\n",
            "Total timesteps:221000 - Episode num:221 - Reward:1869.33203799036\n",
            "Total timesteps:222000 - Episode num:222 - Reward:1540.3876873539868\n",
            "Total timesteps:223000 - Episode num:223 - Reward:1574.182421318695\n",
            "Total timesteps:224000 - Episode num:224 - Reward:1331.6675870052493\n",
            "Total timesteps:225000 - Episode num:225 - Reward:1610.8136571231253\n",
            "Average award over 10 is: 1574.9276544348425\n",
            "Total timesteps:226000 - Episode num:226 - Reward:1436.6928411683004\n",
            "Total timesteps:227000 - Episode num:227 - Reward:1513.2043139516704\n",
            "Total timesteps:228000 - Episode num:228 - Reward:1410.2061786065722\n",
            "Total timesteps:229000 - Episode num:229 - Reward:1648.7202486515466\n",
            "Total timesteps:230000 - Episode num:230 - Reward:1738.089221758692\n",
            "Average award over 10 is: 1683.0322847203086\n",
            "Total timesteps:231000 - Episode num:231 - Reward:1705.8045222048618\n",
            "Total timesteps:232000 - Episode num:232 - Reward:1681.7572675642452\n",
            "Total timesteps:233000 - Episode num:233 - Reward:1422.28820764776\n",
            "Total timesteps:234000 - Episode num:234 - Reward:906.3018367098745\n",
            "Total timesteps:235000 - Episode num:235 - Reward:703.3992386278918\n",
            "Average award over 10 is: 1380.2839870986631\n",
            "Total timesteps:236000 - Episode num:236 - Reward:916.2516595388112\n",
            "Total timesteps:237000 - Episode num:237 - Reward:1600.2610001580947\n",
            "Total timesteps:238000 - Episode num:238 - Reward:1719.9277387293798\n",
            "Total timesteps:239000 - Episode num:239 - Reward:1678.319829551144\n",
            "Total timesteps:240000 - Episode num:240 - Reward:801.8995231544695\n",
            "Average award over 10 is: 1187.608316486685\n",
            "Total timesteps:241000 - Episode num:241 - Reward:1131.7527858822193\n",
            "Total timesteps:242000 - Episode num:242 - Reward:1121.8834266648241\n",
            "Total timesteps:243000 - Episode num:243 - Reward:1471.4400092388014\n",
            "Total timesteps:244000 - Episode num:244 - Reward:1272.8263904933742\n",
            "Total timesteps:245000 - Episode num:245 - Reward:1736.5389247767343\n",
            "Average award over 10 is: 1670.1813669834696\n",
            "Total timesteps:246000 - Episode num:246 - Reward:1628.8320598485557\n",
            "Total timesteps:247000 - Episode num:247 - Reward:1346.8403610912496\n",
            "Total timesteps:248000 - Episode num:248 - Reward:1524.764544710718\n",
            "Total timesteps:249000 - Episode num:249 - Reward:1619.7718829956793\n",
            "Total timesteps:250000 - Episode num:250 - Reward:1864.6357602495916\n",
            "Average award over 10 is: 1389.5763267290188\n",
            "Total timesteps:251000 - Episode num:251 - Reward:1514.62154281527\n",
            "Total timesteps:252000 - Episode num:252 - Reward:1881.879786012263\n",
            "Total timesteps:253000 - Episode num:253 - Reward:1470.2725955957862\n",
            "Total timesteps:254000 - Episode num:254 - Reward:1812.3850393021544\n",
            "Total timesteps:255000 - Episode num:255 - Reward:1539.267390935507\n",
            "Average award over 10 is: 1643.7719238870995\n",
            "Total timesteps:256000 - Episode num:256 - Reward:1752.5501945253034\n",
            "Total timesteps:257000 - Episode num:257 - Reward:1625.001913915273\n",
            "Total timesteps:258000 - Episode num:258 - Reward:2094.9662570931387\n",
            "Total timesteps:259000 - Episode num:259 - Reward:1577.5386064872446\n",
            "Total timesteps:260000 - Episode num:260 - Reward:1493.8751906027458\n",
            "Average award over 10 is: 1563.572437566228\n",
            "Total timesteps:261000 - Episode num:261 - Reward:1349.175202956505\n",
            "Total timesteps:262000 - Episode num:262 - Reward:1634.5035755296167\n",
            "Total timesteps:263000 - Episode num:263 - Reward:1651.9580834243134\n",
            "Total timesteps:264000 - Episode num:264 - Reward:1326.6418182472553\n",
            "Total timesteps:265000 - Episode num:265 - Reward:1684.4199446248324\n",
            "Average award over 10 is: 1819.1317610942292\n",
            "Total timesteps:266000 - Episode num:266 - Reward:1738.3922933121291\n",
            "Total timesteps:267000 - Episode num:267 - Reward:1827.5587377898223\n",
            "Total timesteps:268000 - Episode num:268 - Reward:1640.8283204971021\n",
            "Total timesteps:269000 - Episode num:269 - Reward:1754.2153994056148\n",
            "Total timesteps:270000 - Episode num:270 - Reward:1521.2794710599953\n",
            "Average award over 10 is: 1707.0426667360073\n",
            "Total timesteps:271000 - Episode num:271 - Reward:1609.8732138832206\n",
            "Total timesteps:272000 - Episode num:272 - Reward:1734.3929004800386\n",
            "Total timesteps:273000 - Episode num:273 - Reward:1367.9286767780718\n",
            "Total timesteps:274000 - Episode num:274 - Reward:1584.8788161797504\n",
            "Total timesteps:275000 - Episode num:275 - Reward:1648.7662247154367\n",
            "Average award over 10 is: 1723.9869722854332\n",
            "Total timesteps:276000 - Episode num:276 - Reward:1681.600931193675\n",
            "Total timesteps:277000 - Episode num:277 - Reward:1770.8765079559043\n",
            "Total timesteps:278000 - Episode num:278 - Reward:1732.9351473849065\n",
            "Total timesteps:279000 - Episode num:279 - Reward:1791.5865941711995\n",
            "Total timesteps:280000 - Episode num:280 - Reward:1868.2601022677\n",
            "Average award over 10 is: 1670.2753326033348\n",
            "Total timesteps:281000 - Episode num:281 - Reward:1712.6287573804748\n",
            "Total timesteps:282000 - Episode num:282 - Reward:1708.8498883502846\n",
            "Total timesteps:283000 - Episode num:283 - Reward:1564.344577121714\n",
            "Total timesteps:284000 - Episode num:284 - Reward:1865.6143678433139\n",
            "Total timesteps:285000 - Episode num:285 - Reward:1362.220979593094\n",
            "Average award over 10 is: 1827.1274469360826\n",
            "Total timesteps:286000 - Episode num:286 - Reward:1843.3284875915692\n",
            "Total timesteps:287000 - Episode num:287 - Reward:1729.8698463010894\n",
            "Total timesteps:288000 - Episode num:288 - Reward:1649.0532129883234\n",
            "Total timesteps:289000 - Episode num:289 - Reward:1723.013671997389\n",
            "Total timesteps:290000 - Episode num:290 - Reward:1683.571797295876\n",
            "Average award over 10 is: 1740.0330855189204\n",
            "Total timesteps:291000 - Episode num:291 - Reward:1887.594306905326\n",
            "Total timesteps:292000 - Episode num:292 - Reward:1691.2348992563363\n",
            "Total timesteps:293000 - Episode num:293 - Reward:1575.691719448365\n",
            "Total timesteps:294000 - Episode num:294 - Reward:1779.1867053684477\n",
            "Total timesteps:295000 - Episode num:295 - Reward:1748.1175601298007\n",
            "Average award over 10 is: 1620.0853671968216\n",
            "Total timesteps:296000 - Episode num:296 - Reward:1641.4227776199023\n",
            "Total timesteps:297000 - Episode num:297 - Reward:1931.4094086268426\n",
            "Total timesteps:298000 - Episode num:298 - Reward:1836.253845393731\n",
            "Total timesteps:299000 - Episode num:299 - Reward:1695.074182545199\n",
            "Total timesteps:300000 - Episode num:300 - Reward:-136.68906185257174\n",
            "Average award over 10 is: 1720.6228974620583\n",
            "Total timesteps:301000 - Episode num:301 - Reward:1674.3136543234873\n",
            "Total timesteps:302000 - Episode num:302 - Reward:1775.3408988434796\n",
            "Total timesteps:303000 - Episode num:303 - Reward:1697.580371572866\n",
            "Total timesteps:304000 - Episode num:304 - Reward:1551.3625366283713\n",
            "Total timesteps:305000 - Episode num:305 - Reward:1775.0299633173827\n",
            "Average award over 10 is: 1798.4628542952585\n",
            "Total timesteps:306000 - Episode num:306 - Reward:1821.8975442252245\n",
            "Total timesteps:307000 - Episode num:307 - Reward:1958.9387162374046\n",
            "Total timesteps:308000 - Episode num:308 - Reward:1707.2909280011647\n",
            "Total timesteps:309000 - Episode num:309 - Reward:1814.4083849571116\n",
            "Total timesteps:310000 - Episode num:310 - Reward:1734.7545937988182\n",
            "Average award over 10 is: 1962.1884727083914\n",
            "Total timesteps:311000 - Episode num:311 - Reward:1846.862355475132\n",
            "Total timesteps:312000 - Episode num:312 - Reward:1816.6488383598116\n",
            "Total timesteps:313000 - Episode num:313 - Reward:1749.1240950886834\n",
            "Total timesteps:314000 - Episode num:314 - Reward:1723.7105602465886\n",
            "Total timesteps:315000 - Episode num:315 - Reward:1716.1223866384557\n",
            "Average award over 10 is: 1901.96172576403\n",
            "Total timesteps:316000 - Episode num:316 - Reward:1878.5531361714948\n",
            "Total timesteps:317000 - Episode num:317 - Reward:1886.156257710747\n",
            "Total timesteps:318000 - Episode num:318 - Reward:1792.6092163062538\n",
            "Total timesteps:319000 - Episode num:319 - Reward:1750.5873857719425\n",
            "Total timesteps:320000 - Episode num:320 - Reward:1730.5103438198294\n",
            "Average award over 10 is: 1846.453773005711\n",
            "Total timesteps:321000 - Episode num:321 - Reward:1790.6658467063671\n",
            "Total timesteps:322000 - Episode num:322 - Reward:1664.6678985862243\n",
            "Total timesteps:323000 - Episode num:323 - Reward:1686.4745380453635\n",
            "Total timesteps:324000 - Episode num:324 - Reward:1684.719490711282\n",
            "Total timesteps:325000 - Episode num:325 - Reward:1669.5126149770804\n",
            "Average award over 10 is: 1673.000014554388\n",
            "Total timesteps:326000 - Episode num:326 - Reward:1616.48332007025\n",
            "Total timesteps:327000 - Episode num:327 - Reward:1932.962427473007\n",
            "Total timesteps:328000 - Episode num:328 - Reward:1915.1013105683978\n",
            "Total timesteps:329000 - Episode num:329 - Reward:1968.6828886219537\n",
            "Total timesteps:330000 - Episode num:330 - Reward:1810.8671597784762\n",
            "Average award over 10 is: 1915.3465774477365\n",
            "Total timesteps:331000 - Episode num:331 - Reward:1917.1838763739859\n",
            "Total timesteps:332000 - Episode num:332 - Reward:1809.6781818338106\n",
            "Total timesteps:333000 - Episode num:333 - Reward:1792.617919806099\n",
            "Total timesteps:334000 - Episode num:334 - Reward:2077.1463327140636\n",
            "Total timesteps:335000 - Episode num:335 - Reward:2046.597200726327\n",
            "Average award over 10 is: 1907.6162660069162\n",
            "Total timesteps:336000 - Episode num:336 - Reward:1869.8347973327154\n",
            "Total timesteps:337000 - Episode num:337 - Reward:1885.2177116027635\n",
            "Total timesteps:338000 - Episode num:338 - Reward:1889.5160197868427\n",
            "Total timesteps:339000 - Episode num:339 - Reward:2000.68078688341\n",
            "Total timesteps:340000 - Episode num:340 - Reward:1839.4969409470386\n",
            "Average award over 10 is: 1812.5893738008294\n",
            "Total timesteps:341000 - Episode num:341 - Reward:1655.715126387029\n",
            "Total timesteps:342000 - Episode num:342 - Reward:2001.614378872015\n",
            "Total timesteps:343000 - Episode num:343 - Reward:2033.4552944152008\n",
            "Total timesteps:344000 - Episode num:344 - Reward:1668.7540729932134\n",
            "Total timesteps:345000 - Episode num:345 - Reward:1814.4930312642175\n",
            "Average award over 10 is: 2059.6117856200813\n",
            "Total timesteps:346000 - Episode num:346 - Reward:2129.0678075305905\n",
            "Total timesteps:347000 - Episode num:347 - Reward:1917.253894775864\n",
            "Total timesteps:348000 - Episode num:348 - Reward:1825.6641009161794\n",
            "Total timesteps:349000 - Episode num:349 - Reward:1913.1232609722042\n",
            "Total timesteps:350000 - Episode num:350 - Reward:1986.6768174716706\n",
            "Average award over 10 is: 1702.8769595230328\n",
            "Total timesteps:351000 - Episode num:351 - Reward:1652.3571078860075\n",
            "Total timesteps:352000 - Episode num:352 - Reward:1986.079624307102\n",
            "Total timesteps:353000 - Episode num:353 - Reward:1913.9231387418322\n",
            "Total timesteps:354000 - Episode num:354 - Reward:1798.134401907331\n",
            "Total timesteps:355000 - Episode num:355 - Reward:2002.3341379975961\n",
            "Average award over 10 is: 2004.6700552571485\n",
            "Total timesteps:356000 - Episode num:356 - Reward:1914.5289311767765\n",
            "Total timesteps:357000 - Episode num:357 - Reward:1940.0532736171679\n",
            "Total timesteps:358000 - Episode num:358 - Reward:1972.6510256847857\n",
            "Total timesteps:359000 - Episode num:359 - Reward:1993.6056243301534\n",
            "Total timesteps:360000 - Episode num:360 - Reward:1981.210057960862\n",
            "Average award over 10 is: 1979.85377082719\n",
            "Total timesteps:361000 - Episode num:361 - Reward:2020.6030933851239\n",
            "Total timesteps:362000 - Episode num:362 - Reward:1727.1046696784856\n",
            "Total timesteps:363000 - Episode num:363 - Reward:1970.196353168712\n",
            "Total timesteps:364000 - Episode num:364 - Reward:1948.3915766307207\n",
            "Total timesteps:365000 - Episode num:365 - Reward:1931.7478343801263\n",
            "Average award over 10 is: 2042.6363735864525\n",
            "Total timesteps:366000 - Episode num:366 - Reward:1855.194053729365\n",
            "Total timesteps:367000 - Episode num:367 - Reward:1836.8295926833764\n",
            "Total timesteps:368000 - Episode num:368 - Reward:1945.2332843888255\n",
            "Total timesteps:369000 - Episode num:369 - Reward:1871.6624451848745\n",
            "Total timesteps:370000 - Episode num:370 - Reward:1800.7114188476862\n",
            "Average award over 10 is: 1977.0646648560837\n",
            "Total timesteps:371000 - Episode num:371 - Reward:2040.2016852733273\n",
            "Total timesteps:372000 - Episode num:372 - Reward:1976.7538517326725\n",
            "Total timesteps:373000 - Episode num:373 - Reward:1939.2518511775127\n",
            "Total timesteps:374000 - Episode num:374 - Reward:1705.4854008939421\n",
            "Total timesteps:375000 - Episode num:375 - Reward:1993.7487453776441\n",
            "Average award over 10 is: 1952.4786147743987\n",
            "Total timesteps:376000 - Episode num:376 - Reward:1873.8436790671255\n",
            "Total timesteps:377000 - Episode num:377 - Reward:1985.330403741624\n",
            "Total timesteps:378000 - Episode num:378 - Reward:2018.5615538258328\n",
            "Total timesteps:379000 - Episode num:379 - Reward:1869.5418813284082\n",
            "Total timesteps:380000 - Episode num:380 - Reward:2179.727960116409\n",
            "Average award over 10 is: 1997.9815538661362\n",
            "Total timesteps:381000 - Episode num:381 - Reward:2067.5373673621534\n",
            "Total timesteps:382000 - Episode num:382 - Reward:1908.2395990506172\n",
            "Total timesteps:383000 - Episode num:383 - Reward:1804.9183922728123\n",
            "Total timesteps:384000 - Episode num:384 - Reward:1876.5726530471495\n",
            "Total timesteps:385000 - Episode num:385 - Reward:2018.1863535112725\n",
            "Average award over 10 is: 2138.165287611174\n",
            "Total timesteps:386000 - Episode num:386 - Reward:1923.2204408098166\n",
            "Total timesteps:387000 - Episode num:387 - Reward:2087.5663527852043\n",
            "Total timesteps:388000 - Episode num:388 - Reward:2069.8230419169545\n",
            "Total timesteps:389000 - Episode num:389 - Reward:1989.443087378124\n",
            "Total timesteps:390000 - Episode num:390 - Reward:1803.8625308929036\n",
            "Average award over 10 is: 2159.271629758972\n",
            "Total timesteps:391000 - Episode num:391 - Reward:2107.5954875489588\n",
            "Total timesteps:392000 - Episode num:392 - Reward:1878.5657763709303\n",
            "Total timesteps:393000 - Episode num:393 - Reward:1879.12239518249\n",
            "Total timesteps:394000 - Episode num:394 - Reward:1835.3125896516863\n",
            "Total timesteps:395000 - Episode num:395 - Reward:2068.8317995307516\n",
            "Average award over 10 is: 1951.5583860184258\n",
            "Total timesteps:396000 - Episode num:396 - Reward:1792.5517611602534\n",
            "Total timesteps:397000 - Episode num:397 - Reward:1962.9502458367092\n",
            "Total timesteps:398000 - Episode num:398 - Reward:2018.5207299043345\n",
            "Total timesteps:399000 - Episode num:399 - Reward:1997.656473594618\n",
            "Total timesteps:400000 - Episode num:400 - Reward:1952.283843608711\n",
            "Average award over 10 is: 2041.1433862914187\n",
            "Total timesteps:401000 - Episode num:401 - Reward:2069.8178559874136\n",
            "Total timesteps:402000 - Episode num:402 - Reward:2012.729269390845\n",
            "Total timesteps:403000 - Episode num:403 - Reward:1774.9561215980148\n",
            "Total timesteps:404000 - Episode num:404 - Reward:1867.12538803224\n",
            "Total timesteps:405000 - Episode num:405 - Reward:1916.0778683380022\n",
            "Average award over 10 is: 1920.9577833799099\n",
            "Total timesteps:406000 - Episode num:406 - Reward:1930.9609627484444\n",
            "Total timesteps:407000 - Episode num:407 - Reward:2160.554144785449\n",
            "Total timesteps:408000 - Episode num:408 - Reward:2016.3391026541613\n",
            "Total timesteps:409000 - Episode num:409 - Reward:1917.3802387353176\n",
            "Total timesteps:410000 - Episode num:410 - Reward:1931.4060180452227\n",
            "Average award over 10 is: 2090.0098717179003\n",
            "Total timesteps:411000 - Episode num:411 - Reward:2036.7911339931052\n",
            "Total timesteps:412000 - Episode num:412 - Reward:1935.921433209943\n",
            "Total timesteps:413000 - Episode num:413 - Reward:1919.936957098372\n",
            "Total timesteps:414000 - Episode num:414 - Reward:1700.9338416346338\n",
            "Total timesteps:415000 - Episode num:415 - Reward:1912.1096883314585\n",
            "Average award over 10 is: 2148.5965231418763\n",
            "Total timesteps:416000 - Episode num:416 - Reward:2077.736560569403\n",
            "Total timesteps:417000 - Episode num:417 - Reward:1700.858108338322\n",
            "Total timesteps:418000 - Episode num:418 - Reward:1873.136767279988\n",
            "Total timesteps:419000 - Episode num:419 - Reward:1805.0667649729103\n",
            "Total timesteps:420000 - Episode num:420 - Reward:2085.38833605436\n",
            "Average award over 10 is: 1901.1468012288829\n",
            "Total timesteps:421000 - Episode num:421 - Reward:1787.574464645123\n",
            "Total timesteps:422000 - Episode num:422 - Reward:2206.9808446050247\n",
            "Total timesteps:423000 - Episode num:423 - Reward:2223.064383264304\n",
            "Total timesteps:424000 - Episode num:424 - Reward:2171.4887866642484\n",
            "Total timesteps:425000 - Episode num:425 - Reward:2103.9092867349796\n",
            "Average award over 10 is: 1987.1532259109376\n",
            "Total timesteps:426000 - Episode num:426 - Reward:2042.0494996123798\n",
            "Total timesteps:427000 - Episode num:427 - Reward:2035.0941892882881\n",
            "Total timesteps:428000 - Episode num:428 - Reward:2065.8036485781213\n",
            "Total timesteps:429000 - Episode num:429 - Reward:2067.929069327505\n",
            "Total timesteps:430000 - Episode num:430 - Reward:2219.2119925005195\n",
            "Average award over 10 is: 2239.2580198736187\n",
            "Total timesteps:431000 - Episode num:431 - Reward:2108.759952958751\n",
            "Total timesteps:432000 - Episode num:432 - Reward:2135.691117889872\n",
            "Total timesteps:433000 - Episode num:433 - Reward:2153.979810085994\n",
            "Total timesteps:434000 - Episode num:434 - Reward:1959.8853914022318\n",
            "Total timesteps:435000 - Episode num:435 - Reward:1967.6097398272698\n",
            "Average award over 10 is: 2102.5671476121215\n",
            "Total timesteps:436000 - Episode num:436 - Reward:2065.1477523045164\n",
            "Total timesteps:437000 - Episode num:437 - Reward:2184.8642094394927\n",
            "Total timesteps:438000 - Episode num:438 - Reward:2231.7826743101004\n",
            "Total timesteps:439000 - Episode num:439 - Reward:2098.772368009496\n",
            "Total timesteps:440000 - Episode num:440 - Reward:2133.444280436592\n",
            "Average award over 10 is: 2098.4463167452186\n",
            "Total timesteps:441000 - Episode num:441 - Reward:2044.7173192276962\n",
            "Total timesteps:442000 - Episode num:442 - Reward:2243.4314008084393\n",
            "Total timesteps:443000 - Episode num:443 - Reward:2183.5699308067574\n",
            "Total timesteps:444000 - Episode num:444 - Reward:2249.747912550607\n",
            "Total timesteps:445000 - Episode num:445 - Reward:2171.07738986889\n",
            "Average award over 10 is: 2095.2539697545662\n",
            "Total timesteps:446000 - Episode num:446 - Reward:1931.8299697499326\n",
            "Total timesteps:447000 - Episode num:447 - Reward:2212.684583001473\n",
            "Total timesteps:448000 - Episode num:448 - Reward:2232.602854700779\n",
            "Total timesteps:449000 - Episode num:449 - Reward:2260.919274096359\n",
            "Total timesteps:450000 - Episode num:450 - Reward:2179.6741520941328\n",
            "Average award over 10 is: 2218.9614411746247\n",
            "Total timesteps:451000 - Episode num:451 - Reward:2071.7349803750585\n",
            "Total timesteps:452000 - Episode num:452 - Reward:2111.6253146420927\n",
            "Total timesteps:453000 - Episode num:453 - Reward:2098.409739122497\n",
            "Total timesteps:454000 - Episode num:454 - Reward:2171.801698859386\n",
            "Total timesteps:455000 - Episode num:455 - Reward:2075.8377431263575\n",
            "Average award over 10 is: 2224.216574170364\n",
            "Total timesteps:456000 - Episode num:456 - Reward:2253.3308968754923\n",
            "Total timesteps:457000 - Episode num:457 - Reward:2275.2847921297744\n",
            "Total timesteps:458000 - Episode num:458 - Reward:2261.8735518441536\n",
            "Total timesteps:459000 - Episode num:459 - Reward:2252.626271863395\n",
            "Total timesteps:460000 - Episode num:460 - Reward:2267.7894017964018\n",
            "Average award over 10 is: 1903.2604003730492\n",
            "Total timesteps:461000 - Episode num:461 - Reward:1836.4384072755738\n",
            "Total timesteps:462000 - Episode num:462 - Reward:2255.2910842613032\n",
            "Total timesteps:463000 - Episode num:463 - Reward:1927.8848794966532\n",
            "Total timesteps:464000 - Episode num:464 - Reward:1965.8974313818385\n",
            "Total timesteps:465000 - Episode num:465 - Reward:2082.610709597513\n",
            "Average award over 10 is: 1813.0486394512932\n",
            "Total timesteps:466000 - Episode num:466 - Reward:1906.465541929405\n",
            "Total timesteps:467000 - Episode num:467 - Reward:2088.502201157654\n",
            "Total timesteps:468000 - Episode num:468 - Reward:2239.4507238974147\n",
            "Total timesteps:469000 - Episode num:469 - Reward:2169.108327834476\n",
            "Total timesteps:470000 - Episode num:470 - Reward:2077.6021346449165\n",
            "Average award over 10 is: 2121.4336295506073\n",
            "Total timesteps:471000 - Episode num:471 - Reward:2066.2663865311565\n",
            "Total timesteps:472000 - Episode num:472 - Reward:2136.5984493079563\n",
            "Total timesteps:473000 - Episode num:473 - Reward:2015.330802164041\n",
            "Total timesteps:474000 - Episode num:474 - Reward:2112.3806988505835\n",
            "Total timesteps:475000 - Episode num:475 - Reward:2056.183008603786\n",
            "Average award over 10 is: 2226.5634880201596\n",
            "Total timesteps:476000 - Episode num:476 - Reward:2140.8294378104174\n",
            "Total timesteps:477000 - Episode num:477 - Reward:2139.4699670906834\n",
            "Total timesteps:478000 - Episode num:478 - Reward:1988.4810477632984\n",
            "Total timesteps:479000 - Episode num:479 - Reward:2060.578146808384\n",
            "Total timesteps:480000 - Episode num:480 - Reward:2002.6086166118218\n",
            "Average award over 10 is: 1884.6896722288752\n",
            "Total timesteps:481000 - Episode num:481 - Reward:1828.1036280997193\n",
            "Total timesteps:482000 - Episode num:482 - Reward:1987.0556729773205\n",
            "Total timesteps:483000 - Episode num:483 - Reward:2291.8743489238323\n",
            "Total timesteps:484000 - Episode num:484 - Reward:2168.1872173581987\n",
            "Total timesteps:485000 - Episode num:485 - Reward:2017.2322163301906\n",
            "Average award over 10 is: 2263.0937769935153\n",
            "Total timesteps:486000 - Episode num:486 - Reward:2174.2574379586076\n",
            "Total timesteps:487000 - Episode num:487 - Reward:2280.9198174470444\n",
            "Total timesteps:488000 - Episode num:488 - Reward:2066.194281968375\n",
            "Total timesteps:489000 - Episode num:489 - Reward:2316.952746735919\n",
            "Total timesteps:490000 - Episode num:490 - Reward:2200.7363062402133\n",
            "Average award over 10 is: 2221.529828761183\n",
            "Total timesteps:491000 - Episode num:491 - Reward:2231.7233401366016\n",
            "Total timesteps:492000 - Episode num:492 - Reward:2116.194853882026\n",
            "Total timesteps:493000 - Episode num:493 - Reward:2147.750986277401\n",
            "Total timesteps:494000 - Episode num:494 - Reward:2237.4073087720208\n",
            "Total timesteps:495000 - Episode num:495 - Reward:2226.948301955346\n",
            "Average award over 10 is: 2361.1275092934166\n",
            "Total timesteps:496000 - Episode num:496 - Reward:2306.027442104748\n",
            "Total timesteps:497000 - Episode num:497 - Reward:2064.1329694548604\n",
            "Total timesteps:498000 - Episode num:498 - Reward:2155.023978025296\n",
            "Total timesteps:499000 - Episode num:499 - Reward:2185.144036345067\n",
            "Total timesteps:500000 - Episode num:500 - Reward:2115.2276740225902\n",
            "Average award over 10 is: 2277.8189848096813\n",
            "Total timesteps:501000 - Episode num:501 - Reward:2151.4892729120647\n",
            "Total timesteps:502000 - Episode num:502 - Reward:2207.351552083087\n",
            "Total timesteps:503000 - Episode num:503 - Reward:2108.685190835732\n",
            "Total timesteps:504000 - Episode num:504 - Reward:2207.6543714734444\n",
            "Total timesteps:505000 - Episode num:505 - Reward:2271.5051937259686\n",
            "Average award over 10 is: 2290.3446020259057\n",
            "Total timesteps:506000 - Episode num:506 - Reward:2286.694211018314\n",
            "Total timesteps:507000 - Episode num:507 - Reward:2170.529881317403\n",
            "Total timesteps:508000 - Episode num:508 - Reward:2323.4518087786823\n",
            "Total timesteps:509000 - Episode num:509 - Reward:2173.307144331877\n",
            "Total timesteps:510000 - Episode num:510 - Reward:2052.2029994993873\n",
            "Average award over 10 is: 2139.4898907471797\n",
            "Total timesteps:511000 - Episode num:511 - Reward:2142.289043683187\n",
            "Total timesteps:512000 - Episode num:512 - Reward:2230.2934682563045\n",
            "Total timesteps:513000 - Episode num:513 - Reward:2180.873405171287\n",
            "Total timesteps:514000 - Episode num:514 - Reward:2269.505847211725\n",
            "Total timesteps:515000 - Episode num:515 - Reward:2217.480412765799\n",
            "Average award over 10 is: 2235.5260641460995\n",
            "Total timesteps:516000 - Episode num:516 - Reward:2137.6217946399033\n",
            "Total timesteps:517000 - Episode num:517 - Reward:2247.6699045083315\n",
            "Total timesteps:518000 - Episode num:518 - Reward:1850.2393687208776\n",
            "Total timesteps:519000 - Episode num:519 - Reward:2140.871842176189\n",
            "Total timesteps:520000 - Episode num:520 - Reward:2189.206999053452\n",
            "Average award over 10 is: 2235.0690275374172\n",
            "Total timesteps:521000 - Episode num:521 - Reward:2164.2392492616286\n",
            "Total timesteps:522000 - Episode num:522 - Reward:2234.3241088980003\n",
            "Total timesteps:523000 - Episode num:523 - Reward:2064.2325649864993\n",
            "Total timesteps:524000 - Episode num:524 - Reward:2182.765042893963\n",
            "Total timesteps:525000 - Episode num:525 - Reward:2198.6391816977566\n",
            "Average award over 10 is: 2247.0106486250493\n",
            "Total timesteps:526000 - Episode num:526 - Reward:2243.6475235077587\n",
            "Total timesteps:527000 - Episode num:527 - Reward:2049.782280504061\n",
            "Total timesteps:528000 - Episode num:528 - Reward:2250.0080362417857\n",
            "Total timesteps:529000 - Episode num:529 - Reward:2282.352086105335\n",
            "Total timesteps:530000 - Episode num:530 - Reward:2164.6355858447587\n",
            "Average award over 10 is: 2203.933562722052\n",
            "Total timesteps:531000 - Episode num:531 - Reward:2221.5593721557675\n",
            "Total timesteps:532000 - Episode num:532 - Reward:2280.2940167537768\n",
            "Total timesteps:533000 - Episode num:533 - Reward:2256.188484635788\n",
            "Total timesteps:534000 - Episode num:534 - Reward:2215.886229667668\n",
            "Total timesteps:535000 - Episode num:535 - Reward:2217.592240018267\n",
            "Average award over 10 is: 2239.865119951756\n",
            "Total timesteps:536000 - Episode num:536 - Reward:2153.643713463144\n",
            "Total timesteps:537000 - Episode num:537 - Reward:2170.6582947455504\n",
            "Total timesteps:538000 - Episode num:538 - Reward:2251.758037333464\n",
            "Total timesteps:539000 - Episode num:539 - Reward:2233.201930257443\n",
            "Total timesteps:540000 - Episode num:540 - Reward:2118.3714041286685\n",
            "Average award over 10 is: 2245.592415983503\n",
            "Total timesteps:541000 - Episode num:541 - Reward:2219.7220908051477\n",
            "Total timesteps:542000 - Episode num:542 - Reward:2169.800942025459\n",
            "Total timesteps:543000 - Episode num:543 - Reward:2096.4354473688895\n",
            "Total timesteps:544000 - Episode num:544 - Reward:2296.5282642717093\n",
            "Total timesteps:545000 - Episode num:545 - Reward:2184.374407697584\n",
            "Average award over 10 is: 2276.249187278215\n",
            "Total timesteps:546000 - Episode num:546 - Reward:2236.774541094583\n",
            "Total timesteps:547000 - Episode num:547 - Reward:2197.1068000419905\n",
            "Total timesteps:548000 - Episode num:548 - Reward:2282.0499128055553\n",
            "Total timesteps:549000 - Episode num:549 - Reward:2257.8184751576246\n",
            "Total timesteps:550000 - Episode num:550 - Reward:2189.3224540003744\n",
            "Average award over 10 is: 2237.619014498556\n",
            "Total timesteps:551000 - Episode num:551 - Reward:2274.6602114430916\n",
            "Total timesteps:552000 - Episode num:552 - Reward:2308.3490376068985\n",
            "Total timesteps:553000 - Episode num:553 - Reward:2191.6181202837124\n",
            "Total timesteps:554000 - Episode num:554 - Reward:2027.016410004713\n",
            "Total timesteps:555000 - Episode num:555 - Reward:1949.4800501600441\n",
            "Average award over 10 is: 2290.9963090554047\n",
            "Total timesteps:556000 - Episode num:556 - Reward:2258.959817328298\n",
            "Total timesteps:557000 - Episode num:557 - Reward:2112.2790814619275\n",
            "Total timesteps:558000 - Episode num:558 - Reward:2193.1808002314424\n",
            "Total timesteps:559000 - Episode num:559 - Reward:2293.5613393896797\n",
            "Total timesteps:560000 - Episode num:560 - Reward:2256.656294114539\n",
            "Average award over 10 is: 2286.9200627625323\n",
            "Total timesteps:561000 - Episode num:561 - Reward:2223.024628107474\n",
            "Total timesteps:562000 - Episode num:562 - Reward:2270.4353366570167\n",
            "Total timesteps:563000 - Episode num:563 - Reward:2189.7699963137597\n",
            "Total timesteps:564000 - Episode num:564 - Reward:2172.3063528704056\n",
            "Total timesteps:565000 - Episode num:565 - Reward:2209.342348018557\n",
            "Average award over 10 is: 2210.585453420647\n",
            "Total timesteps:566000 - Episode num:566 - Reward:2180.1114479398607\n",
            "Total timesteps:567000 - Episode num:567 - Reward:2247.947850590818\n",
            "Total timesteps:568000 - Episode num:568 - Reward:2259.8964748025305\n",
            "Total timesteps:569000 - Episode num:569 - Reward:2250.4442180187593\n",
            "Total timesteps:570000 - Episode num:570 - Reward:2309.207820247106\n",
            "Average award over 10 is: 2299.689183271842\n",
            "Total timesteps:571000 - Episode num:571 - Reward:2314.730366468677\n",
            "Total timesteps:572000 - Episode num:572 - Reward:1962.1169838001958\n",
            "Total timesteps:573000 - Episode num:573 - Reward:2137.8320596711187\n",
            "Total timesteps:574000 - Episode num:574 - Reward:2227.7414514493844\n",
            "Total timesteps:575000 - Episode num:575 - Reward:2073.3498545302964\n",
            "Average award over 10 is: 2305.018017405044\n",
            "Total timesteps:576000 - Episode num:576 - Reward:2245.9933707284863\n",
            "Total timesteps:577000 - Episode num:577 - Reward:2105.10193049256\n",
            "Total timesteps:578000 - Episode num:578 - Reward:2257.773759938712\n",
            "Total timesteps:579000 - Episode num:579 - Reward:2294.878781207374\n",
            "Total timesteps:580000 - Episode num:580 - Reward:2228.4920259141422\n",
            "Average award over 10 is: 2188.839381724353\n",
            "Total timesteps:581000 - Episode num:581 - Reward:2145.9392341317143\n",
            "Total timesteps:582000 - Episode num:582 - Reward:2223.898286457962\n",
            "Total timesteps:583000 - Episode num:583 - Reward:2229.369569300268\n",
            "Total timesteps:584000 - Episode num:584 - Reward:2190.0839972017898\n",
            "Total timesteps:585000 - Episode num:585 - Reward:2238.9730547413187\n",
            "Average award over 10 is: 2194.713533865103\n",
            "Total timesteps:586000 - Episode num:586 - Reward:2085.9118923930832\n",
            "Total timesteps:587000 - Episode num:587 - Reward:2184.9900790756683\n",
            "Total timesteps:588000 - Episode num:588 - Reward:2221.226821761364\n",
            "Total timesteps:589000 - Episode num:589 - Reward:2289.682048555378\n",
            "Total timesteps:590000 - Episode num:590 - Reward:2236.2838806971454\n",
            "Average award over 10 is: 2263.4808741441325\n",
            "Total timesteps:591000 - Episode num:591 - Reward:2166.1287031503225\n",
            "Total timesteps:592000 - Episode num:592 - Reward:2230.1288963474026\n",
            "Total timesteps:593000 - Episode num:593 - Reward:2168.3468730985005\n",
            "Total timesteps:594000 - Episode num:594 - Reward:2301.9289399549284\n",
            "Total timesteps:595000 - Episode num:595 - Reward:2311.0907341271914\n",
            "Average award over 10 is: 2224.545666246072\n",
            "Total timesteps:596000 - Episode num:596 - Reward:2226.8948347314004\n",
            "Total timesteps:597000 - Episode num:597 - Reward:2215.5153199241545\n",
            "Total timesteps:598000 - Episode num:598 - Reward:2203.4696721950804\n",
            "Total timesteps:599000 - Episode num:599 - Reward:2137.669543887727\n",
            "Average award over 10 is: 2333.665560118171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test  "
      ],
      "metadata": {
        "id": "fpYFDyi11O6Z"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pybullet"
      ],
      "metadata": {
        "id": "aGIKRBgWaXe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "ZuFkSYESaWPp"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr+1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition) \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        sample_data = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "\n",
        "        states_ = []\n",
        "        next_states_ = [] \n",
        "        actions_ = []\n",
        "        rewards_ = []\n",
        "        dones_= []\n",
        "        for i in sample_data:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            states_.append(state)\n",
        "            next_states_.append(next_state)\n",
        "            actions_.append(action)\n",
        "            rewards_.append(reward)\n",
        "            dones_.append(done)\n",
        "        \n",
        "        return np.array(states_), np.array(next_states_), np.array(actions_), np.array(rewards_).reshape(-1,1), np.array(dones_).reshape(-1,1)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input, action, cut):#action is the number outputs | output is the number of actions\n",
        "        super(Actor,self).__init__()\n",
        "        self.fully01 = nn.Linear(input, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.last = nn.Linear(300, action)\n",
        "        self.cut = cut\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fully01(x))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        return self.cut * torch.tanh(self.last(x))#the cut to adjust to the output levels. higher or lower that -1,1                             \n",
        "\n",
        "#since we need two pair of critics, im making both on the same class.\n",
        "#the name of the class should be DoubleCritic,PairCritic..etc\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input, action):\n",
        "        super(Critic, self).__init__()\n",
        "        #first\n",
        "        self.fully01 = nn.Linear(input+action, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.fully03 = nn.Linear(300, 1)\n",
        "\n",
        "        #second\n",
        "        self.fully11 = nn.Linear(input+action, 400)\n",
        "        self.fully22 = nn.Linear(400,300)\n",
        "        self.fully33 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        #first\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        #Second\n",
        "        y = F.relu(self.fully11(xu))\n",
        "        y = F.relu(self.fully22(y))\n",
        "        y = self.fully33(y)\n",
        "        return x, y\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        return x\n",
        "\n",
        "#training\n",
        "class TD3(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        # actors\n",
        "        self.Actor = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target.load_state_dict(self.Actor.state_dict())\n",
        "\n",
        "        # actor optimizer\n",
        "        self.Actor_optimizer = torch.optim.Adam(self.Actor.parameters())\n",
        "\n",
        "        ## Critic \n",
        "        self.Critic = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target.load_state_dict(self.Critic.state_dict())\n",
        "\n",
        "        ## Critic optimizer\n",
        "        self.Critic_optimizer = torch.optim.Adam(self.Critic.parameters())\n",
        "\n",
        "        ### Max_Action is the cut/clip\n",
        "        self.max_action =  max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1,-1)).to(DEVICE)\n",
        "        return self.Actor(state).cpu().data.numpy().flatten()\n",
        "        #return self.Actor(state).data.numpy().flatten()\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.Actor.state_dict(),'%s/%s_Actor.pth' % (directory,filename))\n",
        "        torch.save(self.Critic.state_dict(),'%s/%s_Critic.pth' % (directory,filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.Actor.load_state_dict(torch.load('%s/%s_Actor.pth' % (directory,filename)))\n",
        "        self.Critic.load_state_dict(torch.load('%s/%s_Critic.pth' % (directory,filename)))\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for i in range(iterations):\n",
        "            states_, next_states_, actions_, rewards_, dones_ = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.Tensor(states_).to(DEVICE)\n",
        "            next_state = torch.Tensor(next_states_).to(DEVICE)\n",
        "            action = torch.Tensor(actions_).to(DEVICE)\n",
        "            reward = torch.Tensor(rewards_).to(DEVICE)\n",
        "            done = torch.Tensor(dones_).to(DEVICE)\n",
        "\n",
        "            next_action = self.Actor_Target(next_state)\n",
        "\n",
        "            noise = torch.Tensor(actions_).data.normal_(0, policy_noise).to(DEVICE)\n",
        "            noise = noise.clamp(-noise_clip,noise_clip)\n",
        "            next_action = (next_action+noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            Target_Q1, Target_Q2 = self.Critic_Target(next_state, next_action)\n",
        "            \n",
        "            #when episode is over 1, not over 0. \n",
        "            # we detached because adding the reward which is the output \n",
        "            #of nn to the computaional graph would not be what we want.\n",
        "            Target_Q = torch.min(Target_Q1, Target_Q2)\n",
        "            Target_Q = reward + (discount * Target_Q * (1 - done)).detach()\n",
        "          \n",
        "            Current_Q1, Current_Q2 = self.Critic(state, action)\n",
        "            critic_loss = F.mse_loss(Current_Q1,Target_Q) + F.mse_loss(Current_Q2, Target_Q)\n",
        "\n",
        "            self.Critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.Critic_optimizer.step()\n",
        "\n",
        "            if not i % policy_freq:\n",
        "                actor_loss = -self.Critic.Q1(state, self.Actor(state)).mean()\n",
        "                self.Actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.Actor_optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.Actor.parameters(), self.Actor_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "                \n",
        "                for param, target_param in zip(self.Critic.parameters(), self.Critic_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "def evaluate_policy(policy, episodes=10):\n",
        "    avg_awards = 0\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_awards += reward\n",
        "    avg_awards /= episodes\n",
        "    print(f\"Average award over {episodes} is:\",avg_awards)\n",
        "    return avg_awards\n",
        "\n",
        "def mkdir(base, name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "metadata": {
        "id": "RFvqs75A1O8Y"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'HalfCheetahBulletEnv-v0'\n",
        "seed = 0\n",
        "file_name = f\"TD3--{env_name}--seed({seed})\"\n",
        "print(file_name)\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "\n",
        "max_episode_step = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "    env.reset()\n",
        "#setting env\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "#agent\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, \"./pytorch_models/\")\n",
        "_ = evaluate_policy(policy, episodes=eval_episodes)"
      ],
      "metadata": {
        "id": "UxWlrx49KRPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09115080-7878-4bd6-f360-9ba2d8b27637"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD3--HalfCheetahBulletEnv-v0--seed(0)\n",
            "Average award over 10 is: 2348.1127026957147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUv3MtVrzhTM",
        "outputId": "985c4101-4909-499b-85a6-ada1c2666a4c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XOh6btBSz78q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}