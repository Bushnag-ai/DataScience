{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ant.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pybullet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB6SkSb78G6_",
        "outputId": "f9df72bb-b9f3-47f2-eb97-235107dd0be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBGrjvQs5ZoU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('GPU on:', True if torch.cuda.is_available() else False, '| Device:',DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRTSDnr3WCm9",
        "outputId": "aa975d4e-9dc6-430e-8ab6-7e1ae92086db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU on: True | Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr+1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition) \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        sample_data = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "\n",
        "        states_ = []\n",
        "        next_states_ = [] \n",
        "        actions_ = []\n",
        "        rewards_ = []\n",
        "        dones_= []\n",
        "        for i in sample_data:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            states_.append(state)\n",
        "            next_states_.append(next_state)\n",
        "            actions_.append(action)\n",
        "            rewards_.append(reward)\n",
        "            dones_.append(done)\n",
        "        \n",
        "        return np.array(states_), np.array(next_states_), np.array(actions_), np.array(rewards_).reshape(-1,1), np.array(dones_).reshape(-1,1)"
      ],
      "metadata": {
        "id": "MAzkCTVL8Cz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input, action, cut):#action is the number outputs | output is the number of actions\n",
        "        super(Actor,self).__init__()\n",
        "        self.fully01 = nn.Linear(input, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.last = nn.Linear(300, action)\n",
        "        self.cut = cut\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fully01(x))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        return self.cut * torch.tanh(self.last(x))#the cut to adjust to the output levels. higher or lower that -1,1                             "
      ],
      "metadata": {
        "id": "zaWQLv_E-19F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since we need two pair of critics, im making both on the same class.\n",
        "#the name of the class should be DoubleCritic,PairCritic..etc\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input, action):\n",
        "        super(Critic, self).__init__()\n",
        "        #first\n",
        "        self.fully01 = nn.Linear(input+action, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.fully03 = nn.Linear(300, 1)\n",
        "\n",
        "        #second\n",
        "        self.fully11 = nn.Linear(input+action, 400)\n",
        "        self.fully22 = nn.Linear(400,300)\n",
        "        self.fully33 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        #first\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        #Second\n",
        "        y = F.relu(self.fully11(xu))\n",
        "        y = F.relu(self.fully22(y))\n",
        "        y = self.fully33(y)\n",
        "        return x, y\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NgrMw3mE-24P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "class TD3(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        # actors\n",
        "        self.Actor = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target.load_state_dict(self.Actor.state_dict())\n",
        "\n",
        "        # actor optimizer\n",
        "        self.Actor_optimizer = torch.optim.Adam(self.Actor.parameters())\n",
        "\n",
        "        ## Critic \n",
        "        self.Critic = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target.load_state_dict(self.Critic.state_dict())\n",
        "\n",
        "        ## Critic optimizer\n",
        "        self.Critic_optimizer = torch.optim.Adam(self.Critic.parameters())\n",
        "\n",
        "        ### Max_Action is the cut/clip\n",
        "        self.max_action =  max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1,-1)).to(DEVICE)\n",
        "        return self.Actor(state).cpu().data.numpy().flatten()\n",
        "        #return self.Actor(state).data.numpy().flatten()\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.Actor.state_dict(),'%s/%s_Actor.pth' % (directory,filename))\n",
        "        torch.save(self.Critic.state_dict(),'%s/%s_Critic.pth' % (directory,filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.Actor.load_state_dict(torch.load('%s/%s_Actor.pth' % (directory,filename)))\n",
        "        self.Critic.load_state_dict(torch.load('%s/%s_Critic.pth' % (directory,filename)))\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for i in range(iterations):\n",
        "            states_, next_states_, actions_, rewards_, dones_ = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.Tensor(states_).to(DEVICE)\n",
        "            next_state = torch.Tensor(next_states_).to(DEVICE)\n",
        "            action = torch.Tensor(actions_).to(DEVICE)\n",
        "            reward = torch.Tensor(rewards_).to(DEVICE)\n",
        "            done = torch.Tensor(dones_).to(DEVICE)\n",
        "\n",
        "            next_action = self.Actor_Target(next_state)\n",
        "\n",
        "            noise = torch.Tensor(actions_).data.normal_(0, policy_noise).to(DEVICE)\n",
        "            noise = noise.clamp(-noise_clip,noise_clip)\n",
        "            next_action = (next_action+noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            Target_Q1, Target_Q2 = self.Critic_Target(next_state, next_action)\n",
        "            \n",
        "            #when episode is over 1, not over 0. \n",
        "            # we detached because adding the reward which is the output \n",
        "            #of nn to the computaional graph would not be what we want.\n",
        "            Target_Q = torch.min(Target_Q1, Target_Q2)\n",
        "            Target_Q = reward + (discount * Target_Q * (1 - done)).detach()\n",
        "          \n",
        "            Current_Q1, Current_Q2 = self.Critic(state, action)\n",
        "            critic_loss = F.mse_loss(Current_Q1,Target_Q) + F.mse_loss(Current_Q2, Target_Q)\n",
        "\n",
        "            self.Critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.Critic_optimizer.step()\n",
        "\n",
        "            if not i % policy_freq:\n",
        "                actor_loss = -self.Critic.Q1(state, self.Actor(state)).mean()\n",
        "                self.Actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.Actor_optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.Actor.parameters(), self.Actor_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "                \n",
        "                for param, target_param in zip(self.Critic.parameters(), self.Critic_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)"
      ],
      "metadata": {
        "id": "YvN3Pdw2VKXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy, episodes=10):\n",
        "    avg_awards = 0\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_awards += reward\n",
        "    avg_awards /= episodes\n",
        "    print(f\"Average award over {episodes} is:\",avg_awards)\n",
        "    return avg_awards"
      ],
      "metadata": {
        "id": "uRc2bSmh91_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "env_name = 'AntBulletEnv-v0'\n",
        "seed = 0\n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3\n",
        "max_timesteps= 6e5\n",
        "\n",
        "save_model = True\n",
        "expi_noise = 0.1\n",
        "batch_size = 100\n",
        "discount = 0.99\n",
        "tau = 0.005\n",
        "\n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5\n",
        "policy_freq = 2"
      ],
      "metadata": {
        "id": "syR2bCx7VKce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "U6NG65vZVKej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = f\"TD3--{env_name}--seed({seed})\"\n",
        "print(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iPqcoSTvxYR",
        "outputId": "bed7330c-fec1-4c97-e9bd-700d5fab7d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD3--AntBulletEnv-v0--seed(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('./results'):\n",
        "    os.makedirs('./results')\n",
        "if save_model and not os.path.exists('./pytorch_models'):\n",
        "    os.makedirs('./pytorch_models')"
      ],
      "metadata": {
        "id": "lULFXN0t0bci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting env\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "metadata": {
        "id": "9u7zaZkd1Oqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "metadata": {
        "id": "hafzat8h1Osy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = ReplayBuffer() "
      ],
      "metadata": {
        "id": "KQv8Y-Q_1Ou4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = [evaluate_policy(policy)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug1NGAnn1OxH",
        "outputId": "2ead71f8-a9e3-4421-b53e-5d4aec6c5c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average award over 10 is: 9.804959632589021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "    \n",
        "work_dir = mkdir('exp','brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_step = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "G1HcVeyp1OzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing the variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "VCpTqgBc1O1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while total_timesteps < max_timesteps:\n",
        "    if done:\n",
        "        if total_timesteps != 0:\n",
        "            print(f'Total timesteps:{total_timesteps} - Episode num:{episode_num} - Reward:{episode_reward}')\n",
        "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "        \n",
        "        if timesteps_since_eval >= eval_freq:\n",
        "            timesteps_since_eval %=  eval_freq\n",
        "            evaluation.append(evaluate_policy(policy))\n",
        "            policy.save(file_name, directory='./pytorch_models')\n",
        "            np.save('./results/%s'%(file_name), evaluation)\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    #Before 10000 timesteps,we play Random actions.\n",
        "    if total_timesteps < start_timesteps:\n",
        "        action = env.action_space.sample()\n",
        "    else:#after 10000 we switch to the policy/model/agent\n",
        "        action = policy.select_action(np.array(obs))\n",
        "        if expi_noise != 0:\n",
        "            action = (action + np.random.normal(0, expi_noise, size=env.action_space.shape[0])).clip(env.action_space.low,env.action_space.high)\n",
        "\n",
        "    new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "    episode_reward += reward\n",
        "    \n",
        "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "    obs = new_obs\n",
        "    episode_timesteps += 1\n",
        "    total_timesteps += 1\n",
        "    timesteps_since_eval += 1\n",
        "\n",
        "evaluation.append(evaluate_policy(policy))\n",
        "if save_model:\n",
        "    policy.save('%s'% (file_name), directory='./pytorch_models')\n",
        "np.save(\"./results/%s\" % (file_name),evaluation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RAlOA781O35",
        "outputId": "631e061f-aad3-46e6-9b50-0717251e353a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total timesteps:276 - Episode num:1 - Reward:131.62786826018868\n",
            "Total timesteps:388 - Episode num:2 - Reward:47.01539916544716\n",
            "Total timesteps:1388 - Episode num:3 - Reward:505.4738057953525\n",
            "Total timesteps:2388 - Episode num:4 - Reward:517.1334121491947\n",
            "Total timesteps:3388 - Episode num:5 - Reward:502.2730272369925\n",
            "Total timesteps:4388 - Episode num:6 - Reward:302.3371050317785\n",
            "Total timesteps:5388 - Episode num:7 - Reward:477.58652490362726\n",
            "Average award over 10 is: 132.8667485656455\n",
            "Total timesteps:6388 - Episode num:8 - Reward:508.28018640888496\n",
            "Total timesteps:6408 - Episode num:9 - Reward:4.72980264786602\n",
            "Total timesteps:7408 - Episode num:10 - Reward:532.2012187316641\n",
            "Total timesteps:8408 - Episode num:11 - Reward:511.83091842658433\n",
            "Total timesteps:9408 - Episode num:12 - Reward:506.5792116317387\n",
            "Total timesteps:10408 - Episode num:13 - Reward:517.8055776770134\n",
            "Average award over 10 is: 100.9131392300153\n",
            "Total timesteps:11408 - Episode num:14 - Reward:123.18990455142708\n",
            "Total timesteps:12408 - Episode num:15 - Reward:111.68080348773498\n",
            "Total timesteps:13408 - Episode num:16 - Reward:205.92140837304478\n",
            "Total timesteps:14408 - Episode num:17 - Reward:202.75440490371264\n",
            "Total timesteps:15408 - Episode num:18 - Reward:97.28415545756576\n",
            "Average award over 10 is: 93.9638841351483\n",
            "Total timesteps:16408 - Episode num:19 - Reward:122.48689968087787\n",
            "Total timesteps:17408 - Episode num:20 - Reward:103.32829335264216\n",
            "Total timesteps:18408 - Episode num:21 - Reward:43.19780004720423\n",
            "Total timesteps:18970 - Episode num:22 - Reward:60.34533548893807\n",
            "Total timesteps:19970 - Episode num:23 - Reward:93.08832245458834\n",
            "Total timesteps:20970 - Episode num:24 - Reward:90.60521539465584\n",
            "Average award over 10 is: 126.92479885043879\n",
            "Total timesteps:21970 - Episode num:25 - Reward:157.6760476560997\n",
            "Total timesteps:21990 - Episode num:26 - Reward:-0.6658948642505769\n",
            "Total timesteps:22010 - Episode num:27 - Reward:-0.34261216782867043\n",
            "Total timesteps:22030 - Episode num:28 - Reward:-0.5685615728602706\n",
            "Total timesteps:22050 - Episode num:29 - Reward:-0.03219351790314118\n",
            "Total timesteps:22070 - Episode num:30 - Reward:-0.037479995087591256\n",
            "Total timesteps:22090 - Episode num:31 - Reward:-0.4558197186059303\n",
            "Total timesteps:22110 - Episode num:32 - Reward:0.02767809902680174\n",
            "Total timesteps:22130 - Episode num:33 - Reward:-0.3154393271595435\n",
            "Total timesteps:22150 - Episode num:34 - Reward:-0.19260995580093088\n",
            "Total timesteps:22170 - Episode num:35 - Reward:-0.8958852989021144\n",
            "Total timesteps:22190 - Episode num:36 - Reward:-0.39375050265378864\n",
            "Total timesteps:22210 - Episode num:37 - Reward:-0.4202090984500042\n",
            "Total timesteps:22230 - Episode num:38 - Reward:-0.5275010687835502\n",
            "Total timesteps:22250 - Episode num:39 - Reward:-0.5455096246070821\n",
            "Total timesteps:22270 - Episode num:40 - Reward:-1.2041677418489876\n",
            "Total timesteps:22290 - Episode num:41 - Reward:-1.7937598016694059\n",
            "Total timesteps:22310 - Episode num:42 - Reward:-1.0748819149773752\n",
            "Total timesteps:22330 - Episode num:43 - Reward:-0.3095901938556129\n",
            "Total timesteps:22350 - Episode num:44 - Reward:-1.0808822739998423\n",
            "Total timesteps:23350 - Episode num:45 - Reward:391.42765186307633\n",
            "Total timesteps:23370 - Episode num:46 - Reward:-0.5942372592885592\n",
            "Total timesteps:23442 - Episode num:47 - Reward:15.129958906113197\n",
            "Total timesteps:24442 - Episode num:48 - Reward:121.83149612259803\n",
            "Total timesteps:25442 - Episode num:49 - Reward:208.4964088749057\n",
            "Average award over 10 is: 213.75159135953572\n",
            "Total timesteps:26442 - Episode num:50 - Reward:138.9406139052973\n",
            "Total timesteps:27442 - Episode num:51 - Reward:423.4599860076777\n",
            "Total timesteps:28442 - Episode num:52 - Reward:245.0005389417954\n",
            "Total timesteps:29442 - Episode num:53 - Reward:342.2008749324385\n",
            "Total timesteps:30442 - Episode num:54 - Reward:572.4085886749426\n",
            "Average award over 10 is: 356.74531718692947\n",
            "Total timesteps:31442 - Episode num:55 - Reward:312.8727227920377\n",
            "Total timesteps:32442 - Episode num:56 - Reward:348.2288204192112\n",
            "Total timesteps:33442 - Episode num:57 - Reward:350.7575981836524\n",
            "Total timesteps:34442 - Episode num:58 - Reward:209.98763548612166\n",
            "Total timesteps:35442 - Episode num:59 - Reward:419.42018043014104\n",
            "Average award over 10 is: 113.49284579454897\n",
            "Total timesteps:36442 - Episode num:60 - Reward:114.11295851764521\n",
            "Total timesteps:37442 - Episode num:61 - Reward:293.19964515432434\n",
            "Total timesteps:38442 - Episode num:62 - Reward:260.96008370716515\n",
            "Total timesteps:39442 - Episode num:63 - Reward:153.89299640529703\n",
            "Total timesteps:40442 - Episode num:64 - Reward:390.5896820162519\n",
            "Average award over 10 is: 183.71057440025746\n",
            "Total timesteps:41442 - Episode num:65 - Reward:160.5577151404109\n",
            "Total timesteps:42442 - Episode num:66 - Reward:359.00727399849\n",
            "Total timesteps:43442 - Episode num:67 - Reward:412.8956776874343\n",
            "Total timesteps:44442 - Episode num:68 - Reward:436.9916087314563\n",
            "Total timesteps:45442 - Episode num:69 - Reward:545.6339409482385\n",
            "Average award over 10 is: 448.5253419186647\n",
            "Total timesteps:46442 - Episode num:70 - Reward:513.035977503089\n",
            "Total timesteps:47442 - Episode num:71 - Reward:442.33056491909787\n",
            "Total timesteps:47506 - Episode num:72 - Reward:19.630494571139558\n",
            "Total timesteps:47544 - Episode num:73 - Reward:14.932724900192538\n",
            "Total timesteps:47638 - Episode num:74 - Reward:26.17125510777699\n",
            "Total timesteps:47684 - Episode num:75 - Reward:13.978153472287936\n",
            "Total timesteps:47742 - Episode num:76 - Reward:29.611307731960213\n",
            "Total timesteps:48742 - Episode num:77 - Reward:300.9070369165185\n",
            "Total timesteps:49742 - Episode num:78 - Reward:331.73294567761695\n",
            "Total timesteps:50742 - Episode num:79 - Reward:281.0060802495031\n",
            "Average award over 10 is: 337.8340009103486\n",
            "Total timesteps:51742 - Episode num:80 - Reward:306.82835696667024\n",
            "Total timesteps:52742 - Episode num:81 - Reward:430.3082859133879\n",
            "Total timesteps:53742 - Episode num:82 - Reward:522.3454333239551\n",
            "Total timesteps:54742 - Episode num:83 - Reward:522.5510599412327\n",
            "Total timesteps:55742 - Episode num:84 - Reward:307.8808186858063\n",
            "Average award over 10 is: 386.27081020000776\n",
            "Total timesteps:56742 - Episode num:85 - Reward:397.5587886910112\n",
            "Total timesteps:57742 - Episode num:86 - Reward:508.42952344055226\n",
            "Total timesteps:58742 - Episode num:87 - Reward:766.5508414726685\n",
            "Total timesteps:59742 - Episode num:88 - Reward:428.6255891405353\n",
            "Total timesteps:60742 - Episode num:89 - Reward:534.807230952589\n",
            "Average award over 10 is: 169.4442631628415\n",
            "Total timesteps:60842 - Episode num:90 - Reward:52.90304349218643\n",
            "Total timesteps:60993 - Episode num:91 - Reward:51.18403201107543\n",
            "Total timesteps:61151 - Episode num:92 - Reward:68.54114525086624\n",
            "Total timesteps:61452 - Episode num:93 - Reward:157.84885170425426\n",
            "Total timesteps:61609 - Episode num:94 - Reward:61.04436987301539\n",
            "Total timesteps:61905 - Episode num:95 - Reward:120.15867790119637\n",
            "Total timesteps:62905 - Episode num:96 - Reward:456.1992830789118\n",
            "Total timesteps:62959 - Episode num:97 - Reward:18.793410592678654\n",
            "Total timesteps:63044 - Episode num:98 - Reward:29.26886297639621\n",
            "Total timesteps:64044 - Episode num:99 - Reward:631.0870854511581\n",
            "Total timesteps:65044 - Episode num:100 - Reward:379.3221331638955\n",
            "Average award over 10 is: 432.9659724906087\n",
            "Total timesteps:66044 - Episode num:101 - Reward:492.5888220094286\n",
            "Total timesteps:67044 - Episode num:102 - Reward:341.7607300597452\n",
            "Total timesteps:68044 - Episode num:103 - Reward:635.4971267077699\n",
            "Total timesteps:69044 - Episode num:104 - Reward:514.9283746356496\n",
            "Total timesteps:70044 - Episode num:105 - Reward:744.6855277598461\n",
            "Average award over 10 is: 452.32376083738575\n",
            "Total timesteps:71044 - Episode num:106 - Reward:386.19250269767514\n",
            "Total timesteps:72044 - Episode num:107 - Reward:719.819046671674\n",
            "Total timesteps:73044 - Episode num:108 - Reward:615.449019633751\n",
            "Total timesteps:74044 - Episode num:109 - Reward:640.3042617912357\n",
            "Total timesteps:75044 - Episode num:110 - Reward:386.9236675980206\n",
            "Average award over 10 is: 524.8699338436948\n",
            "Total timesteps:76044 - Episode num:111 - Reward:583.9172583016162\n",
            "Total timesteps:77044 - Episode num:112 - Reward:371.9120232575261\n",
            "Total timesteps:78044 - Episode num:113 - Reward:347.66924590485945\n",
            "Total timesteps:79044 - Episode num:114 - Reward:438.97914562626676\n",
            "Total timesteps:80044 - Episode num:115 - Reward:228.17454273035833\n",
            "Average award over 10 is: 474.6460360633744\n",
            "Total timesteps:81044 - Episode num:116 - Reward:617.1160680942374\n",
            "Total timesteps:82044 - Episode num:117 - Reward:334.45321894785377\n",
            "Total timesteps:83044 - Episode num:118 - Reward:388.9843755629513\n",
            "Total timesteps:83306 - Episode num:119 - Reward:101.47811921170299\n",
            "Total timesteps:84306 - Episode num:120 - Reward:539.277088819813\n",
            "Total timesteps:85306 - Episode num:121 - Reward:468.8793087016206\n",
            "Average award over 10 is: 467.53645607487135\n",
            "Total timesteps:86306 - Episode num:122 - Reward:522.6499472312265\n",
            "Total timesteps:87306 - Episode num:123 - Reward:539.5753686598855\n",
            "Total timesteps:88306 - Episode num:124 - Reward:588.6754899375484\n",
            "Total timesteps:89306 - Episode num:125 - Reward:329.1243121220768\n",
            "Total timesteps:90306 - Episode num:126 - Reward:496.40592344356503\n",
            "Average award over 10 is: 534.8040939733211\n",
            "Total timesteps:91306 - Episode num:127 - Reward:571.5838000626024\n",
            "Total timesteps:92306 - Episode num:128 - Reward:497.74513001266655\n",
            "Total timesteps:93306 - Episode num:129 - Reward:542.0612575239448\n",
            "Total timesteps:94306 - Episode num:130 - Reward:620.5882081562565\n",
            "Total timesteps:95306 - Episode num:131 - Reward:326.72501664247096\n",
            "Average award over 10 is: 493.84802204187434\n",
            "Total timesteps:96306 - Episode num:132 - Reward:559.5409411584219\n",
            "Total timesteps:97306 - Episode num:133 - Reward:558.5816208224729\n",
            "Total timesteps:98306 - Episode num:134 - Reward:346.08367300216315\n",
            "Total timesteps:99306 - Episode num:135 - Reward:513.4213701020049\n",
            "Total timesteps:100306 - Episode num:136 - Reward:462.669015786007\n",
            "Average award over 10 is: 424.53160965510585\n",
            "Total timesteps:101306 - Episode num:137 - Reward:395.3710453111476\n",
            "Total timesteps:102306 - Episode num:138 - Reward:334.8279481244353\n",
            "Total timesteps:103306 - Episode num:139 - Reward:292.96512738656406\n",
            "Total timesteps:104306 - Episode num:140 - Reward:433.6437487619696\n",
            "Total timesteps:105306 - Episode num:141 - Reward:439.33850291079244\n",
            "Average award over 10 is: 513.8713939928964\n",
            "Total timesteps:106306 - Episode num:142 - Reward:577.8371319375627\n",
            "Total timesteps:107306 - Episode num:143 - Reward:633.8273227004963\n",
            "Total timesteps:108306 - Episode num:144 - Reward:612.7559410369795\n",
            "Total timesteps:109306 - Episode num:145 - Reward:616.5900840139524\n",
            "Total timesteps:110306 - Episode num:146 - Reward:357.220589380564\n",
            "Average award over 10 is: 514.7947506023825\n",
            "Total timesteps:111306 - Episode num:147 - Reward:378.25552317742734\n",
            "Total timesteps:112306 - Episode num:148 - Reward:524.9377367703071\n",
            "Total timesteps:113306 - Episode num:149 - Reward:483.57494553464346\n",
            "Total timesteps:114306 - Episode num:150 - Reward:447.80321264164843\n",
            "Total timesteps:115306 - Episode num:151 - Reward:476.1614340791316\n",
            "Average award over 10 is: 435.18795025257225\n",
            "Total timesteps:116306 - Episode num:152 - Reward:459.4869778515252\n",
            "Total timesteps:117306 - Episode num:153 - Reward:362.99295447153975\n",
            "Total timesteps:118306 - Episode num:154 - Reward:568.2064747375991\n",
            "Total timesteps:119306 - Episode num:155 - Reward:542.9547581497203\n",
            "Total timesteps:120306 - Episode num:156 - Reward:646.3386355403948\n",
            "Average award over 10 is: 708.8683458455705\n",
            "Total timesteps:121306 - Episode num:157 - Reward:654.7230236349423\n",
            "Total timesteps:122306 - Episode num:158 - Reward:501.47446221152774\n",
            "Total timesteps:123306 - Episode num:159 - Reward:408.5310721370644\n",
            "Total timesteps:124306 - Episode num:160 - Reward:132.89361796111427\n",
            "Total timesteps:125306 - Episode num:161 - Reward:645.1817640498766\n",
            "Average award over 10 is: 587.2403337783659\n",
            "Total timesteps:126306 - Episode num:162 - Reward:546.156327431958\n",
            "Total timesteps:127306 - Episode num:163 - Reward:498.94942845825716\n",
            "Total timesteps:128306 - Episode num:164 - Reward:361.2040193059867\n",
            "Total timesteps:129306 - Episode num:165 - Reward:460.1658962179036\n",
            "Total timesteps:130306 - Episode num:166 - Reward:728.2517937865648\n",
            "Average award over 10 is: 411.38648324045863\n",
            "Total timesteps:131306 - Episode num:167 - Reward:707.4453662706468\n",
            "Total timesteps:132306 - Episode num:168 - Reward:421.84268004692643\n",
            "Total timesteps:132327 - Episode num:169 - Reward:3.2631529783426436\n",
            "Total timesteps:132507 - Episode num:170 - Reward:63.02330916223316\n",
            "Total timesteps:132530 - Episode num:171 - Reward:3.549987176582526\n",
            "Total timesteps:132551 - Episode num:172 - Reward:3.7728662671358175\n",
            "Total timesteps:133551 - Episode num:173 - Reward:581.7684066662345\n",
            "Total timesteps:134551 - Episode num:174 - Reward:638.2772521666182\n",
            "Total timesteps:134571 - Episode num:175 - Reward:2.6646058911122963\n",
            "Total timesteps:134592 - Episode num:176 - Reward:3.316175224580074\n",
            "Total timesteps:134773 - Episode num:177 - Reward:73.9872766585514\n",
            "Total timesteps:135773 - Episode num:178 - Reward:707.9215488748205\n",
            "Average award over 10 is: 43.36771410611793\n",
            "Total timesteps:136773 - Episode num:179 - Reward:415.07044381531256\n",
            "Total timesteps:136793 - Episode num:180 - Reward:2.4817997277582213\n",
            "Total timesteps:136813 - Episode num:181 - Reward:2.705174130249575\n",
            "Total timesteps:136833 - Episode num:182 - Reward:0.6544034215502013\n",
            "Total timesteps:136853 - Episode num:183 - Reward:2.2933959400110586\n",
            "Total timesteps:137853 - Episode num:184 - Reward:701.0735082882197\n",
            "Total timesteps:138853 - Episode num:185 - Reward:331.48230772414524\n",
            "Total timesteps:139853 - Episode num:186 - Reward:638.445873278997\n",
            "Total timesteps:140853 - Episode num:187 - Reward:580.6538973150826\n",
            "Average award over 10 is: 524.2150855896057\n",
            "Total timesteps:141853 - Episode num:188 - Reward:462.72226096281054\n",
            "Total timesteps:142853 - Episode num:189 - Reward:443.62267403001846\n",
            "Total timesteps:143853 - Episode num:190 - Reward:584.0509311926487\n",
            "Total timesteps:144853 - Episode num:191 - Reward:532.6813553544316\n",
            "Total timesteps:145853 - Episode num:192 - Reward:590.8291675581745\n",
            "Average award over 10 is: 561.8225139314604\n",
            "Total timesteps:146853 - Episode num:193 - Reward:641.730677049262\n",
            "Total timesteps:147853 - Episode num:194 - Reward:650.33347988078\n",
            "Total timesteps:148853 - Episode num:195 - Reward:669.62992002855\n",
            "Total timesteps:149853 - Episode num:196 - Reward:602.5874239431835\n",
            "Total timesteps:150853 - Episode num:197 - Reward:417.3274690113024\n",
            "Average award over 10 is: 561.4159096757578\n",
            "Total timesteps:151853 - Episode num:198 - Reward:584.5456008757017\n",
            "Total timesteps:152853 - Episode num:199 - Reward:528.0133460894702\n",
            "Total timesteps:153853 - Episode num:200 - Reward:467.44663214787994\n",
            "Total timesteps:154853 - Episode num:201 - Reward:616.4127654713654\n",
            "Total timesteps:155853 - Episode num:202 - Reward:650.6271102720879\n",
            "Average award over 10 is: 547.4637438648695\n",
            "Total timesteps:156853 - Episode num:203 - Reward:318.4029188550337\n",
            "Total timesteps:157853 - Episode num:204 - Reward:302.044628440752\n",
            "Total timesteps:158853 - Episode num:205 - Reward:541.974064383276\n",
            "Total timesteps:159853 - Episode num:206 - Reward:606.1585727377337\n",
            "Total timesteps:160853 - Episode num:207 - Reward:634.2736115456061\n",
            "Average award over 10 is: 507.79541930111253\n",
            "Total timesteps:161853 - Episode num:208 - Reward:475.91242343553137\n",
            "Total timesteps:162853 - Episode num:209 - Reward:518.9491791091581\n",
            "Total timesteps:163853 - Episode num:210 - Reward:642.4226027496342\n",
            "Total timesteps:164853 - Episode num:211 - Reward:491.0856178015027\n",
            "Total timesteps:165853 - Episode num:212 - Reward:416.2423458591411\n",
            "Average award over 10 is: 450.43981252919593\n",
            "Total timesteps:166853 - Episode num:213 - Reward:385.9796811625814\n",
            "Total timesteps:167853 - Episode num:214 - Reward:574.6775436698802\n",
            "Total timesteps:168853 - Episode num:215 - Reward:652.9102186585853\n",
            "Total timesteps:169853 - Episode num:216 - Reward:613.4496099573444\n",
            "Total timesteps:170853 - Episode num:217 - Reward:571.8565880124089\n",
            "Average award over 10 is: 593.8824403970698\n",
            "Total timesteps:171853 - Episode num:218 - Reward:642.8235503971526\n",
            "Total timesteps:172853 - Episode num:219 - Reward:562.6551429650804\n",
            "Total timesteps:173853 - Episode num:220 - Reward:401.26340705083896\n",
            "Total timesteps:174853 - Episode num:221 - Reward:442.0037128518232\n",
            "Total timesteps:175853 - Episode num:222 - Reward:499.30884069265915\n",
            "Average award over 10 is: 565.5481492412848\n",
            "Total timesteps:176853 - Episode num:223 - Reward:436.42465749837504\n",
            "Total timesteps:177853 - Episode num:224 - Reward:585.5851848945064\n",
            "Total timesteps:178853 - Episode num:225 - Reward:546.9719402087599\n",
            "Total timesteps:179853 - Episode num:226 - Reward:748.023101390291\n",
            "Total timesteps:180853 - Episode num:227 - Reward:493.287145716338\n",
            "Average award over 10 is: 684.099111438243\n",
            "Total timesteps:181853 - Episode num:228 - Reward:622.8023516760214\n",
            "Total timesteps:182853 - Episode num:229 - Reward:899.3573046800987\n",
            "Total timesteps:183853 - Episode num:230 - Reward:772.0511004121902\n",
            "Total timesteps:184853 - Episode num:231 - Reward:893.4755770301294\n",
            "Total timesteps:185853 - Episode num:232 - Reward:426.5553996088369\n",
            "Average award over 10 is: 585.947730736935\n",
            "Total timesteps:186853 - Episode num:233 - Reward:733.3592036124284\n",
            "Total timesteps:187853 - Episode num:234 - Reward:503.33585825125795\n",
            "Total timesteps:188853 - Episode num:235 - Reward:622.0071852561452\n",
            "Total timesteps:189853 - Episode num:236 - Reward:472.3554311806403\n",
            "Total timesteps:190853 - Episode num:237 - Reward:788.819723851517\n",
            "Average award over 10 is: 615.0358539782144\n",
            "Total timesteps:191853 - Episode num:238 - Reward:636.3170586054645\n",
            "Total timesteps:192853 - Episode num:239 - Reward:723.5193792829999\n",
            "Total timesteps:193853 - Episode num:240 - Reward:726.423648015108\n",
            "Total timesteps:194853 - Episode num:241 - Reward:600.1372686997078\n",
            "Total timesteps:195853 - Episode num:242 - Reward:683.2197013091317\n",
            "Average award over 10 is: 635.7521385889075\n",
            "Total timesteps:196853 - Episode num:243 - Reward:704.4662834087632\n",
            "Total timesteps:197853 - Episode num:244 - Reward:630.5189762813908\n",
            "Total timesteps:198853 - Episode num:245 - Reward:649.0154572431229\n",
            "Total timesteps:199853 - Episode num:246 - Reward:478.36320344258235\n",
            "Total timesteps:200853 - Episode num:247 - Reward:747.7583109403042\n",
            "Average award over 10 is: 714.9174189402057\n",
            "Total timesteps:201853 - Episode num:248 - Reward:698.8488531117587\n",
            "Total timesteps:202853 - Episode num:249 - Reward:794.481845131802\n",
            "Total timesteps:203853 - Episode num:250 - Reward:447.42998090962436\n",
            "Total timesteps:204853 - Episode num:251 - Reward:847.216253800474\n",
            "Total timesteps:205853 - Episode num:252 - Reward:729.5474877712884\n",
            "Average award over 10 is: 696.1507158333062\n",
            "Total timesteps:206853 - Episode num:253 - Reward:683.8486673473222\n",
            "Total timesteps:207853 - Episode num:254 - Reward:769.3977058819125\n",
            "Total timesteps:208853 - Episode num:255 - Reward:346.48745081634826\n",
            "Total timesteps:209853 - Episode num:256 - Reward:689.3215949043152\n",
            "Total timesteps:210853 - Episode num:257 - Reward:634.6224774324428\n",
            "Average award over 10 is: 740.5017532264333\n",
            "Total timesteps:211004 - Episode num:258 - Reward:96.33466193526576\n",
            "Total timesteps:212004 - Episode num:259 - Reward:443.68144020221416\n",
            "Total timesteps:213004 - Episode num:260 - Reward:789.8834810693407\n",
            "Total timesteps:214004 - Episode num:261 - Reward:564.2563818679732\n",
            "Total timesteps:215004 - Episode num:262 - Reward:802.9775065975078\n",
            "Average award over 10 is: 632.0967818629554\n",
            "Total timesteps:216004 - Episode num:263 - Reward:506.83512977770386\n",
            "Total timesteps:217004 - Episode num:264 - Reward:518.954863797091\n",
            "Total timesteps:218004 - Episode num:265 - Reward:649.6578550233265\n",
            "Total timesteps:219004 - Episode num:266 - Reward:415.22885181206203\n",
            "Total timesteps:220004 - Episode num:267 - Reward:764.6081085170931\n",
            "Average award over 10 is: 710.2255549615055\n",
            "Total timesteps:221004 - Episode num:268 - Reward:759.9794982273187\n",
            "Total timesteps:222004 - Episode num:269 - Reward:452.1359906919421\n",
            "Total timesteps:223004 - Episode num:270 - Reward:736.5495358418814\n",
            "Total timesteps:224004 - Episode num:271 - Reward:661.438556629268\n",
            "Total timesteps:225004 - Episode num:272 - Reward:753.0569706021821\n",
            "Average award over 10 is: 715.0830691153753\n",
            "Total timesteps:226004 - Episode num:273 - Reward:672.9125859975278\n",
            "Total timesteps:227004 - Episode num:274 - Reward:789.5194628602139\n",
            "Total timesteps:228004 - Episode num:275 - Reward:680.695361523324\n",
            "Total timesteps:229004 - Episode num:276 - Reward:629.6910140077621\n",
            "Total timesteps:230004 - Episode num:277 - Reward:705.063400135046\n",
            "Average award over 10 is: 530.0840198458892\n",
            "Total timesteps:231004 - Episode num:278 - Reward:421.07662899671664\n",
            "Total timesteps:232004 - Episode num:279 - Reward:560.945624447565\n",
            "Total timesteps:233004 - Episode num:280 - Reward:676.4456334927693\n",
            "Total timesteps:234004 - Episode num:281 - Reward:819.2467914226964\n",
            "Total timesteps:235004 - Episode num:282 - Reward:528.1117281119771\n",
            "Average award over 10 is: 590.7287611688644\n",
            "Total timesteps:236004 - Episode num:283 - Reward:673.4476785157404\n",
            "Total timesteps:237004 - Episode num:284 - Reward:698.8933208778197\n",
            "Total timesteps:238004 - Episode num:285 - Reward:730.4022065827601\n",
            "Total timesteps:239004 - Episode num:286 - Reward:622.1956097052697\n",
            "Total timesteps:240004 - Episode num:287 - Reward:740.6805272383281\n",
            "Average award over 10 is: 685.9492332502815\n",
            "Total timesteps:241004 - Episode num:288 - Reward:692.349658213127\n",
            "Total timesteps:242004 - Episode num:289 - Reward:719.1789288242596\n",
            "Total timesteps:243004 - Episode num:290 - Reward:628.2053000952211\n",
            "Total timesteps:244004 - Episode num:291 - Reward:573.8473747268741\n",
            "Total timesteps:245004 - Episode num:292 - Reward:688.9502979082253\n",
            "Average award over 10 is: 678.5481073229091\n",
            "Total timesteps:246004 - Episode num:293 - Reward:442.82728969376035\n",
            "Total timesteps:247004 - Episode num:294 - Reward:687.9640427757182\n",
            "Total timesteps:248004 - Episode num:295 - Reward:570.1008245978855\n",
            "Total timesteps:249004 - Episode num:296 - Reward:697.883250938197\n",
            "Total timesteps:250004 - Episode num:297 - Reward:683.6417872965735\n",
            "Average award over 10 is: 735.4074911249683\n",
            "Total timesteps:251004 - Episode num:298 - Reward:713.4228527929976\n",
            "Total timesteps:252004 - Episode num:299 - Reward:833.0558778139698\n",
            "Total timesteps:253004 - Episode num:300 - Reward:629.6800802056007\n",
            "Total timesteps:254004 - Episode num:301 - Reward:569.0459549699092\n",
            "Total timesteps:255004 - Episode num:302 - Reward:824.4700835128058\n",
            "Average award over 10 is: 650.0305683309691\n",
            "Total timesteps:256004 - Episode num:303 - Reward:629.6420375010036\n",
            "Total timesteps:257004 - Episode num:304 - Reward:604.728922630953\n",
            "Total timesteps:258004 - Episode num:305 - Reward:501.5260929618717\n",
            "Total timesteps:259004 - Episode num:306 - Reward:634.0888156402218\n",
            "Total timesteps:260004 - Episode num:307 - Reward:699.3417131850379\n",
            "Average award over 10 is: 773.4233588699684\n",
            "Total timesteps:261004 - Episode num:308 - Reward:856.4550273290706\n",
            "Total timesteps:262004 - Episode num:309 - Reward:526.470714015689\n",
            "Total timesteps:263004 - Episode num:310 - Reward:791.7807217081305\n",
            "Total timesteps:264004 - Episode num:311 - Reward:792.5893496716687\n",
            "Total timesteps:265004 - Episode num:312 - Reward:768.5005391497473\n",
            "Average award over 10 is: 546.3018937844442\n",
            "Total timesteps:266004 - Episode num:313 - Reward:727.7064647773077\n",
            "Total timesteps:267004 - Episode num:314 - Reward:516.4857673010032\n",
            "Total timesteps:268004 - Episode num:315 - Reward:461.25367798111756\n",
            "Total timesteps:269004 - Episode num:316 - Reward:758.3687845845716\n",
            "Total timesteps:270004 - Episode num:317 - Reward:538.1940458268854\n",
            "Average award over 10 is: 623.7747238896396\n",
            "Total timesteps:271004 - Episode num:318 - Reward:627.3694850462898\n",
            "Total timesteps:272004 - Episode num:319 - Reward:574.3802162842038\n",
            "Total timesteps:273004 - Episode num:320 - Reward:358.5995251258182\n",
            "Total timesteps:274004 - Episode num:321 - Reward:522.1149221090732\n",
            "Total timesteps:275004 - Episode num:322 - Reward:536.9286801450673\n",
            "Average award over 10 is: 772.0894299832046\n",
            "Total timesteps:276004 - Episode num:323 - Reward:809.233439276509\n",
            "Total timesteps:277004 - Episode num:324 - Reward:555.4027059185847\n",
            "Total timesteps:278004 - Episode num:325 - Reward:786.3780035194372\n",
            "Total timesteps:279004 - Episode num:326 - Reward:734.0562191500107\n",
            "Total timesteps:280004 - Episode num:327 - Reward:826.8618782282939\n",
            "Average award over 10 is: 577.9401405130698\n",
            "Total timesteps:281004 - Episode num:328 - Reward:647.6387877231656\n",
            "Total timesteps:282004 - Episode num:329 - Reward:764.9658039547368\n",
            "Total timesteps:283004 - Episode num:330 - Reward:676.9027052197074\n",
            "Total timesteps:284004 - Episode num:331 - Reward:636.7416606944331\n",
            "Total timesteps:285004 - Episode num:332 - Reward:857.9919069525799\n",
            "Average award over 10 is: 859.6740205096654\n",
            "Total timesteps:286004 - Episode num:333 - Reward:907.0273891820622\n",
            "Total timesteps:287004 - Episode num:334 - Reward:936.3879658570263\n",
            "Total timesteps:288004 - Episode num:335 - Reward:962.1099169515807\n",
            "Total timesteps:289004 - Episode num:336 - Reward:923.7659022780188\n",
            "Total timesteps:290004 - Episode num:337 - Reward:794.5190064689242\n",
            "Average award over 10 is: 811.3397470937396\n",
            "Total timesteps:291004 - Episode num:338 - Reward:706.107361602947\n",
            "Total timesteps:292004 - Episode num:339 - Reward:797.6399147335455\n",
            "Total timesteps:293004 - Episode num:340 - Reward:911.3180105220384\n",
            "Total timesteps:294004 - Episode num:341 - Reward:893.1353292494988\n",
            "Total timesteps:295004 - Episode num:342 - Reward:835.1028280202455\n",
            "Average award over 10 is: 792.210567951683\n",
            "Total timesteps:296004 - Episode num:343 - Reward:649.7769293758137\n",
            "Total timesteps:297004 - Episode num:344 - Reward:851.7479769295043\n",
            "Total timesteps:298004 - Episode num:345 - Reward:870.1362986989507\n",
            "Total timesteps:299004 - Episode num:346 - Reward:906.2558907428348\n",
            "Total timesteps:300004 - Episode num:347 - Reward:888.2031534657773\n",
            "Average award over 10 is: 753.4770514563612\n",
            "Total timesteps:301004 - Episode num:348 - Reward:759.4088305001688\n",
            "Total timesteps:302004 - Episode num:349 - Reward:678.8983941339493\n",
            "Total timesteps:303004 - Episode num:350 - Reward:487.6000912160208\n",
            "Total timesteps:304004 - Episode num:351 - Reward:873.2974374057198\n",
            "Total timesteps:305004 - Episode num:352 - Reward:762.8250779748563\n",
            "Average award over 10 is: 629.402503455169\n",
            "Total timesteps:306004 - Episode num:353 - Reward:759.1266000233604\n",
            "Total timesteps:307004 - Episode num:354 - Reward:798.687162416919\n",
            "Total timesteps:308004 - Episode num:355 - Reward:897.5667119947011\n",
            "Total timesteps:309004 - Episode num:356 - Reward:746.5139390484799\n",
            "Total timesteps:310004 - Episode num:357 - Reward:836.641228410367\n",
            "Average award over 10 is: 814.7048008523282\n",
            "Total timesteps:311004 - Episode num:358 - Reward:836.3715649202129\n",
            "Total timesteps:312004 - Episode num:359 - Reward:863.0725978434476\n",
            "Total timesteps:313004 - Episode num:360 - Reward:779.7033664434181\n",
            "Total timesteps:314004 - Episode num:361 - Reward:881.3148408242897\n",
            "Total timesteps:315004 - Episode num:362 - Reward:865.16656478096\n",
            "Average award over 10 is: 744.6855330536005\n",
            "Total timesteps:316004 - Episode num:363 - Reward:780.3468262839558\n",
            "Total timesteps:317004 - Episode num:364 - Reward:792.5571600132553\n",
            "Total timesteps:318004 - Episode num:365 - Reward:670.2677072918622\n",
            "Total timesteps:319004 - Episode num:366 - Reward:875.6414544214717\n",
            "Total timesteps:320004 - Episode num:367 - Reward:747.0708314086899\n",
            "Average award over 10 is: 865.6552639127738\n",
            "Total timesteps:321004 - Episode num:368 - Reward:866.2371280246556\n",
            "Total timesteps:322004 - Episode num:369 - Reward:916.831548537587\n",
            "Total timesteps:323004 - Episode num:370 - Reward:880.6153890719354\n",
            "Total timesteps:324004 - Episode num:371 - Reward:869.9825520233376\n",
            "Total timesteps:325004 - Episode num:372 - Reward:864.0006209034763\n",
            "Average award over 10 is: 799.128983310264\n",
            "Total timesteps:326004 - Episode num:373 - Reward:799.9549798577839\n",
            "Total timesteps:327004 - Episode num:374 - Reward:826.9781384176661\n",
            "Total timesteps:328004 - Episode num:375 - Reward:814.28084389753\n",
            "Total timesteps:329004 - Episode num:376 - Reward:744.222781922485\n",
            "Total timesteps:330004 - Episode num:377 - Reward:735.0195835057061\n",
            "Average award over 10 is: 771.4191098125763\n",
            "Total timesteps:331004 - Episode num:378 - Reward:821.3049169617907\n",
            "Total timesteps:332004 - Episode num:379 - Reward:709.0579294715681\n",
            "Total timesteps:333004 - Episode num:380 - Reward:805.3816654611135\n",
            "Total timesteps:334004 - Episode num:381 - Reward:672.4001983151082\n",
            "Total timesteps:335004 - Episode num:382 - Reward:776.6715424380476\n",
            "Average award over 10 is: 887.4658368860177\n",
            "Total timesteps:336004 - Episode num:383 - Reward:955.682184784969\n",
            "Total timesteps:337004 - Episode num:384 - Reward:659.3619080874938\n",
            "Total timesteps:338004 - Episode num:385 - Reward:847.2407976534723\n",
            "Total timesteps:339004 - Episode num:386 - Reward:887.5944398815625\n",
            "Total timesteps:340004 - Episode num:387 - Reward:815.2996975984069\n",
            "Average award over 10 is: 842.8562320726712\n",
            "Total timesteps:341004 - Episode num:388 - Reward:855.762552710128\n",
            "Total timesteps:342004 - Episode num:389 - Reward:871.7346971289826\n",
            "Total timesteps:343004 - Episode num:390 - Reward:742.1625154584173\n",
            "Total timesteps:344004 - Episode num:391 - Reward:954.8306622554073\n",
            "Total timesteps:345004 - Episode num:392 - Reward:862.1502142958552\n",
            "Average award over 10 is: 918.3195751300842\n",
            "Total timesteps:346004 - Episode num:393 - Reward:873.7446923023821\n",
            "Total timesteps:347004 - Episode num:394 - Reward:854.9007684526682\n",
            "Total timesteps:348004 - Episode num:395 - Reward:896.4199195278273\n",
            "Total timesteps:349004 - Episode num:396 - Reward:726.3871152499535\n",
            "Total timesteps:350004 - Episode num:397 - Reward:896.8180723213142\n",
            "Average award over 10 is: 834.9524237766836\n",
            "Total timesteps:351004 - Episode num:398 - Reward:961.067488419862\n",
            "Total timesteps:352004 - Episode num:399 - Reward:936.5219199067836\n",
            "Total timesteps:353004 - Episode num:400 - Reward:910.6232281592846\n",
            "Total timesteps:354004 - Episode num:401 - Reward:908.2620322360187\n",
            "Total timesteps:355004 - Episode num:402 - Reward:803.0594561056312\n",
            "Average award over 10 is: 772.7828208581448\n",
            "Total timesteps:356004 - Episode num:403 - Reward:724.1146570113278\n",
            "Total timesteps:357004 - Episode num:404 - Reward:750.4178544042351\n",
            "Total timesteps:358004 - Episode num:405 - Reward:764.0334679261753\n",
            "Total timesteps:359004 - Episode num:406 - Reward:990.7798862461179\n",
            "Total timesteps:360004 - Episode num:407 - Reward:850.0498898725232\n",
            "Average award over 10 is: 790.8881513933864\n",
            "Total timesteps:361004 - Episode num:408 - Reward:842.8895272170103\n",
            "Total timesteps:362004 - Episode num:409 - Reward:915.8097201667509\n",
            "Total timesteps:363004 - Episode num:410 - Reward:967.7508461567982\n",
            "Total timesteps:364004 - Episode num:411 - Reward:821.9891434525248\n",
            "Total timesteps:365004 - Episode num:412 - Reward:995.2940018794541\n",
            "Average award over 10 is: 874.2050753733854\n",
            "Total timesteps:366004 - Episode num:413 - Reward:814.1608673651018\n",
            "Total timesteps:367004 - Episode num:414 - Reward:655.0338458221411\n",
            "Total timesteps:368004 - Episode num:415 - Reward:853.2372734696615\n",
            "Total timesteps:369004 - Episode num:416 - Reward:784.4790292278967\n",
            "Total timesteps:370004 - Episode num:417 - Reward:1003.4665636980487\n",
            "Average award over 10 is: 907.2995451159726\n",
            "Total timesteps:371004 - Episode num:418 - Reward:979.757577365717\n",
            "Total timesteps:372004 - Episode num:419 - Reward:940.5033696456326\n",
            "Total timesteps:373004 - Episode num:420 - Reward:998.6107388851343\n",
            "Total timesteps:374004 - Episode num:421 - Reward:817.1602686098784\n",
            "Total timesteps:375004 - Episode num:422 - Reward:807.1484468280207\n",
            "Average award over 10 is: 942.7887638874103\n",
            "Total timesteps:376004 - Episode num:423 - Reward:1014.0697827275002\n",
            "Total timesteps:377004 - Episode num:424 - Reward:949.052701349592\n",
            "Total timesteps:378004 - Episode num:425 - Reward:983.9338787916332\n",
            "Total timesteps:379004 - Episode num:426 - Reward:871.6621051728396\n",
            "Total timesteps:380004 - Episode num:427 - Reward:941.6785680081365\n",
            "Average award over 10 is: 989.7829454340299\n",
            "Total timesteps:381004 - Episode num:428 - Reward:1008.9792152084833\n",
            "Total timesteps:382004 - Episode num:429 - Reward:940.6984671268106\n",
            "Total timesteps:383004 - Episode num:430 - Reward:812.9500519361067\n",
            "Total timesteps:384004 - Episode num:431 - Reward:889.2941217917707\n",
            "Total timesteps:385004 - Episode num:432 - Reward:907.2427345499027\n",
            "Average award over 10 is: 843.1903243410486\n",
            "Total timesteps:386004 - Episode num:433 - Reward:700.0374021211233\n",
            "Total timesteps:387004 - Episode num:434 - Reward:821.8281758217992\n",
            "Total timesteps:388004 - Episode num:435 - Reward:789.517489585511\n",
            "Total timesteps:389004 - Episode num:436 - Reward:817.4738461230426\n",
            "Total timesteps:390004 - Episode num:437 - Reward:1046.1626097199594\n",
            "Average award over 10 is: 958.4706523695291\n",
            "Total timesteps:391004 - Episode num:438 - Reward:1029.974414771133\n",
            "Total timesteps:392004 - Episode num:439 - Reward:838.034529343979\n",
            "Total timesteps:393004 - Episode num:440 - Reward:959.6692475798951\n",
            "Total timesteps:394004 - Episode num:441 - Reward:949.0787377180278\n",
            "Total timesteps:395004 - Episode num:442 - Reward:873.380797751659\n",
            "Average award over 10 is: 913.4728207865726\n",
            "Total timesteps:396004 - Episode num:443 - Reward:898.3289953610605\n",
            "Total timesteps:397004 - Episode num:444 - Reward:779.5212593355159\n",
            "Total timesteps:398004 - Episode num:445 - Reward:1130.6508720574432\n",
            "Total timesteps:399004 - Episode num:446 - Reward:1223.73337492017\n",
            "Total timesteps:400004 - Episode num:447 - Reward:1100.3860510439818\n",
            "Average award over 10 is: 931.0036315909581\n",
            "Total timesteps:401004 - Episode num:448 - Reward:1245.9872701880172\n",
            "Total timesteps:402004 - Episode num:449 - Reward:1135.1592526468912\n",
            "Total timesteps:403004 - Episode num:450 - Reward:1277.4296831129657\n",
            "Total timesteps:404004 - Episode num:451 - Reward:1220.8983195896315\n",
            "Total timesteps:405004 - Episode num:452 - Reward:1029.41074455929\n",
            "Average award over 10 is: 771.9390814606624\n",
            "Total timesteps:406004 - Episode num:453 - Reward:484.6231766172238\n",
            "Total timesteps:407004 - Episode num:454 - Reward:773.4056233984293\n",
            "Total timesteps:408004 - Episode num:455 - Reward:1028.4040851855866\n",
            "Total timesteps:409004 - Episode num:456 - Reward:1137.9924354232792\n",
            "Total timesteps:410004 - Episode num:457 - Reward:1145.1784434784774\n",
            "Average award over 10 is: 1304.4910883636426\n",
            "Total timesteps:411004 - Episode num:458 - Reward:1217.978364473038\n",
            "Total timesteps:412004 - Episode num:459 - Reward:1451.8682499608324\n",
            "Total timesteps:413004 - Episode num:460 - Reward:1418.1267827855686\n",
            "Total timesteps:414004 - Episode num:461 - Reward:1447.8130395575317\n",
            "Total timesteps:415004 - Episode num:462 - Reward:1504.8352848436996\n",
            "Average award over 10 is: 1396.3640114894042\n",
            "Total timesteps:416004 - Episode num:463 - Reward:1376.9749543281466\n",
            "Total timesteps:417004 - Episode num:464 - Reward:1344.6835066748506\n",
            "Total timesteps:418004 - Episode num:465 - Reward:1346.550683501982\n",
            "Total timesteps:419004 - Episode num:466 - Reward:1428.039049858936\n",
            "Total timesteps:420004 - Episode num:467 - Reward:1154.4003284843752\n",
            "Average award over 10 is: 1371.4367524589384\n",
            "Total timesteps:421004 - Episode num:468 - Reward:1172.8743032634472\n",
            "Total timesteps:422004 - Episode num:469 - Reward:1467.1712088110594\n",
            "Total timesteps:423004 - Episode num:470 - Reward:1346.9860709467507\n",
            "Total timesteps:424004 - Episode num:471 - Reward:1310.4663555036223\n",
            "Total timesteps:425004 - Episode num:472 - Reward:1259.8827362477334\n",
            "Average award over 10 is: 1364.805901345067\n",
            "Total timesteps:426004 - Episode num:473 - Reward:1436.5304257693629\n",
            "Total timesteps:427004 - Episode num:474 - Reward:1370.5122905167025\n",
            "Total timesteps:428004 - Episode num:475 - Reward:1460.7231791303002\n",
            "Total timesteps:429004 - Episode num:476 - Reward:1027.81860597696\n",
            "Total timesteps:430004 - Episode num:477 - Reward:1345.1857293457106\n",
            "Average award over 10 is: 1394.5586879232358\n",
            "Total timesteps:431004 - Episode num:478 - Reward:1399.8746259898724\n",
            "Total timesteps:432004 - Episode num:479 - Reward:1506.726885856748\n",
            "Total timesteps:433004 - Episode num:480 - Reward:1446.6411205096895\n",
            "Total timesteps:434004 - Episode num:481 - Reward:1437.8623126879106\n",
            "Total timesteps:435004 - Episode num:482 - Reward:950.4875077976905\n",
            "Average award over 10 is: 1068.383312203126\n",
            "Total timesteps:436004 - Episode num:483 - Reward:1065.2820905397366\n",
            "Total timesteps:437004 - Episode num:484 - Reward:1336.9591903281232\n",
            "Total timesteps:438004 - Episode num:485 - Reward:1344.3062019799515\n",
            "Total timesteps:439004 - Episode num:486 - Reward:1252.4982483071894\n",
            "Total timesteps:440004 - Episode num:487 - Reward:1407.8974503972474\n",
            "Average award over 10 is: 1306.1156213680156\n",
            "Total timesteps:441004 - Episode num:488 - Reward:1368.3356892627162\n",
            "Total timesteps:442004 - Episode num:489 - Reward:1469.9792886972434\n",
            "Total timesteps:443004 - Episode num:490 - Reward:1452.245898112816\n",
            "Total timesteps:444004 - Episode num:491 - Reward:1464.04915839432\n",
            "Total timesteps:445004 - Episode num:492 - Reward:1507.0317511219419\n",
            "Average award over 10 is: 1484.500387885717\n",
            "Total timesteps:446004 - Episode num:493 - Reward:1459.6696294435126\n",
            "Total timesteps:447004 - Episode num:494 - Reward:1554.7930085405826\n",
            "Total timesteps:448004 - Episode num:495 - Reward:1593.2900895628602\n",
            "Total timesteps:449004 - Episode num:496 - Reward:1582.775034754528\n",
            "Total timesteps:450004 - Episode num:497 - Reward:1464.214374917888\n",
            "Average award over 10 is: 1544.2956334890162\n",
            "Total timesteps:451004 - Episode num:498 - Reward:1544.309095744459\n",
            "Total timesteps:452004 - Episode num:499 - Reward:1558.1057313180913\n",
            "Total timesteps:453004 - Episode num:500 - Reward:842.6279486155142\n",
            "Total timesteps:454004 - Episode num:501 - Reward:1541.067848355035\n",
            "Total timesteps:455004 - Episode num:502 - Reward:1552.8090459420544\n",
            "Average award over 10 is: 1558.3292638010576\n",
            "Total timesteps:456004 - Episode num:503 - Reward:1506.5920214494367\n",
            "Total timesteps:457004 - Episode num:504 - Reward:1531.4601921231572\n",
            "Total timesteps:458004 - Episode num:505 - Reward:1636.6322185232389\n",
            "Total timesteps:459004 - Episode num:506 - Reward:1620.6764951344828\n",
            "Total timesteps:460004 - Episode num:507 - Reward:1497.7861655863444\n",
            "Average award over 10 is: 1548.4369008577728\n",
            "Total timesteps:461004 - Episode num:508 - Reward:1555.9682578274424\n",
            "Total timesteps:462004 - Episode num:509 - Reward:1562.523739314765\n",
            "Total timesteps:463004 - Episode num:510 - Reward:1560.951438485703\n",
            "Total timesteps:464004 - Episode num:511 - Reward:1628.782119841171\n",
            "Total timesteps:465004 - Episode num:512 - Reward:1622.5820572553073\n",
            "Average award over 10 is: 1685.730450262332\n",
            "Total timesteps:466004 - Episode num:513 - Reward:1679.1725193715752\n",
            "Total timesteps:467004 - Episode num:514 - Reward:1599.7711049448774\n",
            "Total timesteps:468004 - Episode num:515 - Reward:1664.737157744275\n",
            "Total timesteps:469004 - Episode num:516 - Reward:1605.7224881395093\n",
            "Total timesteps:470004 - Episode num:517 - Reward:1554.5516279787564\n",
            "Average award over 10 is: 1627.6046243816866\n",
            "Total timesteps:471004 - Episode num:518 - Reward:1591.1443309822864\n",
            "Total timesteps:472004 - Episode num:519 - Reward:1101.4307826735953\n",
            "Total timesteps:473004 - Episode num:520 - Reward:1626.8204957811477\n",
            "Total timesteps:474004 - Episode num:521 - Reward:1604.8138070863886\n",
            "Total timesteps:475004 - Episode num:522 - Reward:1626.3070136388676\n",
            "Average award over 10 is: 1603.4220281207824\n",
            "Total timesteps:476004 - Episode num:523 - Reward:1599.1045409280746\n",
            "Total timesteps:477004 - Episode num:524 - Reward:1556.975152946043\n",
            "Total timesteps:478004 - Episode num:525 - Reward:1679.310536792209\n",
            "Total timesteps:479004 - Episode num:526 - Reward:1677.5081741519743\n",
            "Total timesteps:480004 - Episode num:527 - Reward:1649.985549710295\n",
            "Average award over 10 is: 1688.9753371242023\n",
            "Total timesteps:481004 - Episode num:528 - Reward:1683.8039963283456\n",
            "Total timesteps:482004 - Episode num:529 - Reward:1709.6831443320464\n",
            "Total timesteps:483004 - Episode num:530 - Reward:1687.7945802678487\n",
            "Total timesteps:484004 - Episode num:531 - Reward:1677.9533416740276\n",
            "Total timesteps:485004 - Episode num:532 - Reward:1741.5185684564547\n",
            "Average award over 10 is: 1694.2544018626807\n",
            "Total timesteps:486004 - Episode num:533 - Reward:1691.2868721939399\n",
            "Total timesteps:487004 - Episode num:534 - Reward:1613.4713739348229\n",
            "Total timesteps:488004 - Episode num:535 - Reward:1717.4741723336913\n",
            "Total timesteps:489004 - Episode num:536 - Reward:1755.9580362554311\n",
            "Total timesteps:490004 - Episode num:537 - Reward:1729.8122332190896\n",
            "Average award over 10 is: 1759.893361300415\n",
            "Total timesteps:491004 - Episode num:538 - Reward:1725.9892745648315\n",
            "Total timesteps:492004 - Episode num:539 - Reward:1763.6192682479711\n",
            "Total timesteps:493004 - Episode num:540 - Reward:1694.1427070103632\n",
            "Total timesteps:494004 - Episode num:541 - Reward:1726.4823005176504\n",
            "Total timesteps:495004 - Episode num:542 - Reward:1738.5964529591638\n",
            "Average award over 10 is: 1567.7095911715926\n",
            "Total timesteps:496004 - Episode num:543 - Reward:1594.739303480748\n",
            "Total timesteps:497004 - Episode num:544 - Reward:1674.978961798127\n",
            "Total timesteps:498004 - Episode num:545 - Reward:1661.2091086746898\n",
            "Total timesteps:499004 - Episode num:546 - Reward:1643.9596219551647\n",
            "Total timesteps:500004 - Episode num:547 - Reward:1640.1004879267177\n",
            "Average award over 10 is: 1719.3146078688503\n",
            "Total timesteps:501004 - Episode num:548 - Reward:1728.9743097474075\n",
            "Total timesteps:502004 - Episode num:549 - Reward:1603.9345548465221\n",
            "Total timesteps:503004 - Episode num:550 - Reward:1687.6289307398713\n",
            "Total timesteps:504004 - Episode num:551 - Reward:1611.0199558490947\n",
            "Total timesteps:505004 - Episode num:552 - Reward:1661.9682137565135\n",
            "Average award over 10 is: 1658.3859574755297\n",
            "Total timesteps:506004 - Episode num:553 - Reward:1626.9535163312582\n",
            "Total timesteps:507004 - Episode num:554 - Reward:1729.9347920092923\n",
            "Total timesteps:508004 - Episode num:555 - Reward:1637.8641915945875\n",
            "Total timesteps:509004 - Episode num:556 - Reward:1695.7198625612489\n",
            "Total timesteps:510004 - Episode num:557 - Reward:1708.6585093238607\n",
            "Average award over 10 is: 1721.0236783676228\n",
            "Total timesteps:511004 - Episode num:558 - Reward:1701.1110831017622\n",
            "Total timesteps:512004 - Episode num:559 - Reward:1661.910057567419\n",
            "Total timesteps:513004 - Episode num:560 - Reward:1702.1459705023624\n",
            "Total timesteps:514004 - Episode num:561 - Reward:1793.5808769342527\n",
            "Total timesteps:515004 - Episode num:562 - Reward:1768.4098197174242\n",
            "Average award over 10 is: 1741.365936882629\n",
            "Total timesteps:516004 - Episode num:563 - Reward:1724.24959331108\n",
            "Total timesteps:517004 - Episode num:564 - Reward:1709.9038919170248\n",
            "Total timesteps:518004 - Episode num:565 - Reward:1643.756170600422\n",
            "Total timesteps:519004 - Episode num:566 - Reward:1732.8692515586533\n",
            "Total timesteps:520004 - Episode num:567 - Reward:1694.2458040459355\n",
            "Average award over 10 is: 1751.2253367832013\n",
            "Total timesteps:521004 - Episode num:568 - Reward:1717.0433147598876\n",
            "Total timesteps:522004 - Episode num:569 - Reward:1735.657371810296\n",
            "Total timesteps:523004 - Episode num:570 - Reward:1727.4843711562814\n",
            "Total timesteps:524004 - Episode num:571 - Reward:1709.2716544948717\n",
            "Total timesteps:525004 - Episode num:572 - Reward:1758.5972285856517\n",
            "Average award over 10 is: 1812.1037901342381\n",
            "Total timesteps:526004 - Episode num:573 - Reward:1765.2907432101765\n",
            "Total timesteps:527004 - Episode num:574 - Reward:1769.8796591539378\n",
            "Total timesteps:528004 - Episode num:575 - Reward:1743.555087623815\n",
            "Total timesteps:529004 - Episode num:576 - Reward:1670.6636581193072\n",
            "Total timesteps:530004 - Episode num:577 - Reward:1641.0492703200857\n",
            "Average award over 10 is: 1781.5681584791087\n",
            "Total timesteps:531004 - Episode num:578 - Reward:1741.354702361877\n",
            "Total timesteps:532004 - Episode num:579 - Reward:1589.0317341230855\n",
            "Total timesteps:533004 - Episode num:580 - Reward:1744.0273765278957\n",
            "Total timesteps:534004 - Episode num:581 - Reward:1708.7854374387148\n",
            "Total timesteps:535004 - Episode num:582 - Reward:1692.3570076930678\n",
            "Average award over 10 is: 1795.5631282996935\n",
            "Total timesteps:536004 - Episode num:583 - Reward:1798.9489304659714\n",
            "Total timesteps:537004 - Episode num:584 - Reward:1785.2325470249248\n",
            "Total timesteps:538004 - Episode num:585 - Reward:1181.85002492532\n",
            "Total timesteps:539004 - Episode num:586 - Reward:1725.01657158394\n",
            "Total timesteps:540004 - Episode num:587 - Reward:1734.0084243764504\n",
            "Average award over 10 is: 841.7250892242484\n",
            "Total timesteps:541004 - Episode num:588 - Reward:872.1984904478251\n",
            "Total timesteps:542004 - Episode num:589 - Reward:1420.1487031352399\n",
            "Total timesteps:543004 - Episode num:590 - Reward:1547.316541686532\n",
            "Total timesteps:544004 - Episode num:591 - Reward:1732.762388581548\n",
            "Total timesteps:545004 - Episode num:592 - Reward:1767.2163335609139\n",
            "Average award over 10 is: 1863.0956173147133\n",
            "Total timesteps:546004 - Episode num:593 - Reward:1874.1863677657532\n",
            "Total timesteps:547004 - Episode num:594 - Reward:1870.6105977785057\n",
            "Total timesteps:548004 - Episode num:595 - Reward:1850.9574965877541\n",
            "Total timesteps:549004 - Episode num:596 - Reward:1864.0708319891635\n",
            "Total timesteps:550004 - Episode num:597 - Reward:1866.9654535994182\n",
            "Average award over 10 is: 1904.315905484862\n",
            "Total timesteps:551004 - Episode num:598 - Reward:1878.0908425548464\n",
            "Total timesteps:552004 - Episode num:599 - Reward:1926.2258899772505\n",
            "Total timesteps:553004 - Episode num:600 - Reward:1913.588231749901\n",
            "Total timesteps:554004 - Episode num:601 - Reward:1903.8998254044418\n",
            "Total timesteps:555004 - Episode num:602 - Reward:1923.4754171575821\n",
            "Average award over 10 is: 1834.6840522262137\n",
            "Total timesteps:556004 - Episode num:603 - Reward:1798.2809236636172\n",
            "Total timesteps:557004 - Episode num:604 - Reward:1926.9220247667586\n",
            "Total timesteps:558004 - Episode num:605 - Reward:1833.1871871214157\n",
            "Total timesteps:559004 - Episode num:606 - Reward:1879.1670904262116\n",
            "Total timesteps:560004 - Episode num:607 - Reward:1740.3714636435936\n",
            "Average award over 10 is: 1812.4380391265709\n",
            "Total timesteps:561004 - Episode num:608 - Reward:1769.3126963613756\n",
            "Total timesteps:562004 - Episode num:609 - Reward:1797.6296982079255\n",
            "Total timesteps:563004 - Episode num:610 - Reward:1809.8443736711918\n",
            "Total timesteps:564004 - Episode num:611 - Reward:1675.8923772445162\n",
            "Total timesteps:565004 - Episode num:612 - Reward:1916.0243850962368\n",
            "Average award over 10 is: 1907.7086713540152\n",
            "Total timesteps:566004 - Episode num:613 - Reward:1916.215886965651\n",
            "Total timesteps:567004 - Episode num:614 - Reward:1965.1842093525036\n",
            "Total timesteps:568004 - Episode num:615 - Reward:1867.6455042822763\n",
            "Total timesteps:569004 - Episode num:616 - Reward:1921.5484307800125\n",
            "Total timesteps:570004 - Episode num:617 - Reward:1908.5149727116907\n",
            "Average award over 10 is: 2097.852770283416\n",
            "Total timesteps:571004 - Episode num:618 - Reward:2004.8487095860362\n",
            "Total timesteps:572004 - Episode num:619 - Reward:1974.5518075988332\n",
            "Total timesteps:573004 - Episode num:620 - Reward:1941.9430342741673\n",
            "Total timesteps:574004 - Episode num:621 - Reward:1974.5055403317385\n",
            "Total timesteps:575004 - Episode num:622 - Reward:1971.735519247221\n",
            "Average award over 10 is: 1958.0656319099319\n",
            "Total timesteps:576004 - Episode num:623 - Reward:1954.0327013649928\n",
            "Total timesteps:577004 - Episode num:624 - Reward:1839.0046383197432\n",
            "Total timesteps:578004 - Episode num:625 - Reward:1920.5074634972123\n",
            "Total timesteps:579004 - Episode num:626 - Reward:1955.9757172625614\n",
            "Total timesteps:580004 - Episode num:627 - Reward:2038.163687324777\n",
            "Average award over 10 is: 2113.093098343901\n",
            "Total timesteps:581004 - Episode num:628 - Reward:2058.9109977509574\n",
            "Total timesteps:582004 - Episode num:629 - Reward:2080.9144764630682\n",
            "Total timesteps:583004 - Episode num:630 - Reward:1983.6685854883558\n",
            "Total timesteps:584004 - Episode num:631 - Reward:1841.818731311881\n",
            "Total timesteps:585004 - Episode num:632 - Reward:1888.2136667913871\n",
            "Average award over 10 is: 1903.1934066912677\n",
            "Total timesteps:586004 - Episode num:633 - Reward:1851.881916888635\n",
            "Total timesteps:587004 - Episode num:634 - Reward:1734.2552320016866\n",
            "Total timesteps:588004 - Episode num:635 - Reward:1937.1481168725704\n",
            "Total timesteps:589004 - Episode num:636 - Reward:1825.7829746084465\n",
            "Total timesteps:590004 - Episode num:637 - Reward:1944.3594820903759\n",
            "Average award over 10 is: 1841.6508557951813\n",
            "Total timesteps:591004 - Episode num:638 - Reward:1819.309384966922\n",
            "Total timesteps:592004 - Episode num:639 - Reward:2031.8532628705127\n",
            "Total timesteps:593004 - Episode num:640 - Reward:1975.3896956272206\n",
            "Total timesteps:594004 - Episode num:641 - Reward:1939.9619078118017\n",
            "Total timesteps:595004 - Episode num:642 - Reward:1929.8503898296124\n",
            "Average award over 10 is: 2145.875742064337\n",
            "Total timesteps:596004 - Episode num:643 - Reward:2083.7513169769736\n",
            "Total timesteps:597004 - Episode num:644 - Reward:1891.012583789356\n",
            "Total timesteps:598004 - Episode num:645 - Reward:2117.4187943896586\n",
            "Total timesteps:599004 - Episode num:646 - Reward:2129.277021416117\n",
            "Average award over 10 is: 2129.1926643072293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test"
      ],
      "metadata": {
        "id": "fpYFDyi11O6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pybullet"
      ],
      "metadata": {
        "id": "aGIKRBgWaXe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import pybullet_envs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "ZuFkSYESaWPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size=1e6):\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr+1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition) \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        sample_data = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "\n",
        "        states_ = []\n",
        "        next_states_ = [] \n",
        "        actions_ = []\n",
        "        rewards_ = []\n",
        "        dones_= []\n",
        "        for i in sample_data:\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "            states_.append(state)\n",
        "            next_states_.append(next_state)\n",
        "            actions_.append(action)\n",
        "            rewards_.append(reward)\n",
        "            dones_.append(done)\n",
        "        \n",
        "        return np.array(states_), np.array(next_states_), np.array(actions_), np.array(rewards_).reshape(-1,1), np.array(dones_).reshape(-1,1)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input, action, cut):#action is the number outputs | output is the number of actions\n",
        "        super(Actor,self).__init__()\n",
        "        self.fully01 = nn.Linear(input, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.last = nn.Linear(300, action)\n",
        "        self.cut = cut\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fully01(x))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        return self.cut * torch.tanh(self.last(x))#the cut to adjust to the output levels. higher or lower that -1,1                             \n",
        "\n",
        "#since we need two pair of critics, im making both on the same class.\n",
        "#the name of the class should be DoubleCritic,PairCritic..etc\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input, action):\n",
        "        super(Critic, self).__init__()\n",
        "        #first\n",
        "        self.fully01 = nn.Linear(input+action, 400)\n",
        "        self.fully02 = nn.Linear(400,300)\n",
        "        self.fully03 = nn.Linear(300, 1)\n",
        "\n",
        "        #second\n",
        "        self.fully11 = nn.Linear(input+action, 400)\n",
        "        self.fully22 = nn.Linear(400,300)\n",
        "        self.fully33 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        #first\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        #Second\n",
        "        y = F.relu(self.fully11(xu))\n",
        "        y = F.relu(self.fully22(y))\n",
        "        y = self.fully33(y)\n",
        "        return x, y\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "        xu = torch.cat([x,u], 1)\n",
        "        x = F.relu(self.fully01(xu))\n",
        "        x = F.relu(self.fully02(x))\n",
        "        x = self.fully03(x)\n",
        "        return x\n",
        "\n",
        "#training\n",
        "class TD3(object):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        # actors\n",
        "        self.Actor = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target = Actor(state_dim, action_dim, max_action).to(DEVICE)\n",
        "        self.Actor_Target.load_state_dict(self.Actor.state_dict())\n",
        "\n",
        "        # actor optimizer\n",
        "        self.Actor_optimizer = torch.optim.Adam(self.Actor.parameters())\n",
        "\n",
        "        ## Critic \n",
        "        self.Critic = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target = Critic(state_dim, action_dim).to(DEVICE)\n",
        "        self.Critic_Target.load_state_dict(self.Critic.state_dict())\n",
        "\n",
        "        ## Critic optimizer\n",
        "        self.Critic_optimizer = torch.optim.Adam(self.Critic.parameters())\n",
        "\n",
        "        ### Max_Action is the cut/clip\n",
        "        self.max_action =  max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1,-1)).to(DEVICE)\n",
        "        return self.Actor(state).cpu().data.numpy().flatten()\n",
        "        #return self.Actor(state).data.numpy().flatten()\n",
        "\n",
        "    def save(self, filename, directory):\n",
        "        torch.save(self.Actor.state_dict(),'%s/%s_Actor.pth' % (directory,filename))\n",
        "        torch.save(self.Critic.state_dict(),'%s/%s_Critic.pth' % (directory,filename))\n",
        "\n",
        "    def load(self, filename, directory):\n",
        "        self.Actor.load_state_dict(torch.load('%s/%s_Actor.pth' % (directory,filename)))\n",
        "        self.Critic.load_state_dict(torch.load('%s/%s_Critic.pth' % (directory,filename)))\n",
        "    \n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "        for i in range(iterations):\n",
        "            states_, next_states_, actions_, rewards_, dones_ = replay_buffer.sample(batch_size)\n",
        "\n",
        "            state = torch.Tensor(states_).to(DEVICE)\n",
        "            next_state = torch.Tensor(next_states_).to(DEVICE)\n",
        "            action = torch.Tensor(actions_).to(DEVICE)\n",
        "            reward = torch.Tensor(rewards_).to(DEVICE)\n",
        "            done = torch.Tensor(dones_).to(DEVICE)\n",
        "\n",
        "            next_action = self.Actor_Target(next_state)\n",
        "\n",
        "            noise = torch.Tensor(actions_).data.normal_(0, policy_noise).to(DEVICE)\n",
        "            noise = noise.clamp(-noise_clip,noise_clip)\n",
        "            next_action = (next_action+noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            Target_Q1, Target_Q2 = self.Critic_Target(next_state, next_action)\n",
        "            \n",
        "            #when episode is over 1, not over 0. \n",
        "            # we detached because adding the reward which is the output \n",
        "            #of nn to the computaional graph would not be what we want.\n",
        "            Target_Q = torch.min(Target_Q1, Target_Q2)\n",
        "            Target_Q = reward + (discount * Target_Q * (1 - done)).detach()\n",
        "          \n",
        "            Current_Q1, Current_Q2 = self.Critic(state, action)\n",
        "            critic_loss = F.mse_loss(Current_Q1,Target_Q) + F.mse_loss(Current_Q2, Target_Q)\n",
        "\n",
        "            self.Critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.Critic_optimizer.step()\n",
        "\n",
        "            if not i % policy_freq:\n",
        "                actor_loss = -self.Critic.Q1(state, self.Actor(state)).mean()\n",
        "                self.Actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.Actor_optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.Actor.parameters(), self.Actor_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "                \n",
        "                for param, target_param in zip(self.Critic.parameters(), self.Critic_Target.parameters()):\n",
        "                    target_param.data.copy_(tau*param.data +(1-tau)*target_param.data)\n",
        "                \n",
        "def evaluate_policy(policy, episodes=10):\n",
        "    avg_awards = 0\n",
        "    for _ in range(episodes):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_awards += reward\n",
        "    avg_awards /= episodes\n",
        "    print(f\"Average award over {episodes} is:\",avg_awards)\n",
        "    return avg_awards\n",
        "\n",
        "def mkdir(base, name):\n",
        "    path = os.path.join(base,name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "metadata": {
        "id": "RFvqs75A1O8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'AntBulletEnv-v0'\n",
        "seed = 0\n",
        "file_name = f\"TD3--{env_name}--seed({seed})\"\n",
        "print(file_name)\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "\n",
        "max_episode_step = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force=True)\n",
        "    env.reset()\n",
        "#setting env\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "#agent\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, \"./pytorch_models/\")\n",
        "_ = evaluate_policy(policy, episodes=eval_episodes)"
      ],
      "metadata": {
        "id": "UxWlrx49KRPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30437cc8-c5ed-4d08-e9a6-e2599b50a6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD3--AntBulletEnv-v0--seed(0)\n",
            "Average award over 10 is: 2162.639820566826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmKqggWbxS4B",
        "outputId": "9e438534-0899-4a0a-a5af-d3b38cbe4fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uayLDmx2xpNr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}